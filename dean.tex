\documentclass[11pt]{report}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{braket}
\usepackage{xspace}
\usepackage[showonlyrefs,showmanualtags]{mathtools}
\mathtoolsset{showonlyrefs}
\renewcommand*{\thefootnote}{(\arabic{footnote})}
\newcommand{\psh}[2]{\ensuremath{\langle #1|#2\rangle}\xspace}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\begin{document}
\title{Out of Equilibrium Statistical Physics}
\author{David S. Dean \\ Laboratoire d'Ondes et Mati\`ere d'Aquitaine, Universit\'e de Bordeaux}
\date{2020}                                           
\maketitle
\tableofcontents

\chapter{Stochastic calculus and Langevin equations}
Here we will look at a class of stochastic processes, known as Langevin processes in physics, from an informal probabilistic point of view. The slight effort required, with respect to physical approaches, is worth it as many aspects such as the derivation of the Fokker-Planck equation and first passage times are immediate in this formalism. A basic knowledge of this approach will also be useful in Wall Street or London if you decide that you are more interested in making money than doing physics. The stochastic calculus is the foundation of financial mathematics and the basis of the famous Black-Scholes formula for pricing options.

Here is a list of references that you may find useful

\noindent{\bf Books}

\begin{itemize}
\item M. Le Bellac amd  F. Mortessagne {\em Thermodynamique statistique, \'equilibre et hors \'equilibre}, Dunod (2001).
\item N. Poitier, {\em Physique Statistique hors d'\'equilibre}, EDP Sciences (2007).
\end{itemize}

\noindent{\bf On the web}
\begin{itemize}
\item M. Le Bellac, {\em Non equilibrium statistical mechanics}, Les Houches Lecture N
\item D. Tong {\em Kinetic Theory},  University of Cambridge UK -\\ {\tt http://www.damtp.cam.ac.uk/user/tong/kinetic.html}
\item M. Scott, {\em Applied stochastic processes in science and engineering}, University of Waterloo Canada - {\tt https://www.math.uwaterloo.ca/~mscott/Little\_Notes.pdf}
\end{itemize}


\section{Discrete time continuous space stochastic processes}
Consider a discrete time stochastic process in one dimension (defined at a discrete set of  times $t = n\Delta t$ where $n\in \mathbb Z$) obeying the difference equation
\begin{equation}
X_{(n+1)\Delta t}-X_{n\Delta t}=\Delta X_t = u(X_t) \Delta t + a(X_t) \Delta B_t \label{sded}
\end{equation}
Here $u(x)$ is a local drift field which is deterministic and depends on the particle's position
$X_t$ - it can be due to convection in a fluid for example or the buoyancy force for a fluid particle in the Stokes regime of hydrodynamics (viscous flows). The second term is a noise term due to random fluctuations which are often thermal in nature depending on $k_BT$, for a colloid in a fluid the noise term generates an effective molecular diffusivity which disperses the particle even in the absence of any overall hydrodynamic flow.
The stochastic increment $\Delta B_t$ is Gaussian with  zero mean so
\begin{equation}
\langle \Delta B_t\rangle = 0,
\end{equation}
and we choose the variance to be
\begin{equation}
\langle [\Delta B_t]^2\rangle = \Delta t.
\end{equation}
This means that its probability density function $p(x)$ is given by
\begin{equation}
p(x) dx = {\rm Prob}(\Delta B_t \in [x,x+dx]) = {1\over \sqrt{2\pi \Delta t}}\exp(-{x^2\over 2\Delta t}) dx.
\end{equation}

If the noise has no memory then the fundamental  stochastic increments are statistically independent so
\begin{equation}
\langle\Delta B_t \Delta B_{t'}\rangle = \langle\Delta B_t \rangle
\langle\Delta B_{t'}\rangle = 0
\end{equation}
for two different times $t$ and $t'$. Noise which is uncorrelated in time is referred to as white noise, noise with a non-zero correlation time is referred to as coloured. The hypothesis of white noise requires a separation of times scales, for instance, in colloids the molecular time scales responsible for the noise are indeed much shorter than the times scales for colloidal motion. In the simplest case where $u=0$ and $a$ is constant we can write down the particle position at time $t= n\Delta t$ as
\begin{equation}
X_t = x_0+ a\sum_{i=0}^{n-1} \Delta B_{i\Delta t},
\end{equation}
where $x_0$ is the starting position at time $0$. The easiest way of calculating the distribution of $X_t$, $p(x,t)$ is via its Fourier transform (known as the characteristic function)
\begin{eqnarray}
\tilde p(k,t)&=& \int dx \ p(x,t) \exp(-ikx) = \langle\exp(-i k X_t)\rangle \nonumber \\
&=& \exp(-ik x_0) \prod_{i=1}^n \langle\exp[-i k a\Delta B_{i\Delta t}]\rangle= \exp(ik x_0)\exp(-{a^2\over 2} k^2 n \Delta t).
\end{eqnarray}
Now noting that $n\Delta t= t$ and inverting the Fourier transform we find
\begin{equation}
p(x,t) = {1\over 2\pi}\int dk \exp(ikx)\langle\exp[-i k X_t]\rangle = {1\over \sqrt{2\pi a^2t}}
\exp\left(-{1\over 2a^2 t} (x-x_0)^2\right)
\end{equation}
The continuous diffusion equation in one dimension is 
\begin{equation}
{\partial p(x,t)\over \partial t} = D{\partial ^2\over \partial x^2}p(x,t).
\end{equation}
The term, $D$ has dimensions $L^2/T$ and is called the diffusion constant. If the particle is started at $x_0$ at time $t=0$ the initial condition is $p(x,0) = \delta(x-x_0)$.
The easiest way to solve the diffusion equation is by taking a Fourier transform, so calculate
the characteristic function, to obtain
\begin{equation}
{\partial \tilde p(k,t)\over \partial t}= -Dk^2\tilde p(k,t),
\end{equation}
which has the general solution
\begin{equation}
\tilde p(k,t) = \tilde p(k,0)\exp(-Dk^2 t) = \exp(-ikx_0)\exp(-Dk^2 t).
\end{equation}
This is the same characteristic function as for our discrete model if we identify $D = a^2/2$.
In the continuum limit $\Delta t \to 0$ we have constructed a stochastic process whose statistics are described by the diffusion equation. If we define $\Delta X_t = X_t - x_0$ the displacement of the process $X_t$ with respect to its initial position $x_0$ we have that its characteristic function is given by
\begin{equation}
\langle\exp[-i k \Delta X_t\rangle = \exp(-Dk^2t),
\end{equation}
Taylor expanding both sides to $O(k^2)$ then gives
\begin{equation}
1 -ik \langle\Delta X_t\rangle -\frac{k^2}{2}\langle\Delta X^2_t\rangle = 1- Dk^2 t
\end{equation}
and so we see $\langle\Delta X_t\rangle = 0$, while the mean squared displacement is given by $\langle\Delta X_t^2\rangle = 2D t$.

\section{The Ito Stochastic Calculus}
Here we want to take the continuum limit of the stochastic equation \eqref{sded}. We see that
by construction the noise term $\Delta B_t$ is $O(\sqrt{\Delta t})$ and so the term $\Delta B_t^2$ is $O(\Delta t)$ and so we need to keep it in deriving differential equations. Consider for a moment 
\begin{equation}
S = \sum_{i=1}^N \Delta B_{i\Delta t}^2.
\end{equation}
We find that the mean of $S$ is given by
\begin{equation}
\langle S\rangle = \sum_{i=1}^N  \langle\Delta B_{i\Delta t}^2 \rangle= N\Delta t = t.
\end{equation}
However we can use a specific Wick's theorem for Gaussian random variables to compute 
the averages which appear. 
The simplest case of Wick's theorem is the following. Consider a Gaussian random variable with pdf 
\begin{equation}
p(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp(-\frac{x^2}{2\sigma^2})
\end{equation}
We would like to compute the moments $\langle X^n\rangle$ for integer $n$. To do this we compute the generating function
\begin{eqnarray}
g(\lambda) &=& \langle \exp(\lambda X)\rangle = \int_{-\infty}^{\infty} dx\
 \exp(\lambda x)
p(x) \nonumber \\
&=& \frac{1}{\sqrt{2\pi \sigma^2}}\int_{-\infty}^{\infty} dx\
 \exp(\lambda x)\exp\left[-\frac{x^2}{2\sigma^2}\right] \nonumber\\
 &=& \frac{1}{\sqrt{2\pi \sigma^2}}\int_{-\infty}^{\infty} dx\ 
 \exp\left[(-\frac{(x-\lambda\sigma^2)^2}{2\sigma^2}+\frac{\lambda^2\sigma^2}{2}\right] \nonumber\\
 &=& \frac{1}{\sqrt{2\pi \sigma^2}}\int_{-\infty}^{\infty} dx'\ 
 \exp\left[(-\frac{x'^2}{2\sigma^2}+\frac{\lambda^2\sigma^2}{2}\right] = \exp\left[\frac{\lambda^2\sigma^2}{2}\right].
\end{eqnarray}
However Taylor expanding this $ g(\lambda) = \langle \exp(\lambda X)\rangle$ about $\lambda=0$  gives 
\begin{equation}
\sum_{n=0}^\infty \frac{\lambda^n}{n!} \langle X^n\rangle = \sum_{m=0}^\infty \frac{\lambda^{2m}\sigma^{2m}}{m! 2^m}.
\end{equation}
Now comparing the coefficients of $\lambda$ on both sides of the above we see that
\begin{equation}
\langle X^n\rangle = 0 \ {\rm for}\ n\ {\rm odd},
\end{equation}
and for $n=2m$ even
\begin{equation}
\langle X^{2m}\rangle = \frac{\lambda^{2m} (2m)! \sigma^{2m}}{m! 2^m} = \sigma^{2m} (2m-1)(2m-3)\cdots 1.
\end{equation}
Notice that $(2m-1)(2m-3)\cdots 1$ the number of ways in which $m$ distinct pairs can be formed from $2m$ objects. If you label the objects from $1$ to $2m$ the first object has $2m-1$ choices to be paired with, this leaves $2m-2$ objects, the first of these can then be paired with a possible $2m-3$ objects. 

Wicks theorem for an even number $2m$ of Gaussian random variables, each of average zero, states that
\begin{equation}
\boxed{
\langle A_1A_2\cdots A_{2m}\rangle = \sum_{\rm all pairs} \prod_{\alpha}\langle A_{\alpha}A_{\sigma_\alpha}\rangle.}
\end{equation}
For instance for $4$ Gaussian random variables we have
\begin{equation}
\langle A_1A_2A_3A_4\rangle  = \langle A_1A_2\rangle \langle A_3A_4\rangle  + \langle A_1A_3\rangle  \langle A_2A_4\rangle + \langle A_1A_4\rangle \langle A_2A_3\rangle.
\end{equation}
Now applying Wick's theorem gives
\begin{eqnarray}
\langle S^2\rangle &=& \sum_{ij=1}^N \langle\Delta B_{i\Delta t}^2\Delta B_{j\Delta t}^2 \rangle \nonumber \\
&=&\sum_{ij=1}^N \langle\Delta B_{i\Delta t}^2\rangle \langle\Delta B_{j\Delta t}^2 \rangle+ 2 \langle\Delta B_{i\Delta t}\Delta B_{j\Delta t} \rangle^2 \nonumber \\
&=& \sum_{ij=1}^N \Delta t^2 + 2 \delta_{ij} \Delta t^2
= t^2  +  2 {t^2\over N}
\end{eqnarray}
The variance of $S$ is thus given by
\begin{equation}
{\rm var}(S)= \langle S^2\rangle - \langle S\rangle^2 = 2 {t^2\over N},
\end{equation}
so in the limit of large $N$ this means ${\rm var}(S)\to 0$ and so $S = \langle S\rangle)$
(this is basically the central limit theorem). 
The continuum stochastic differential equation Eq. \eqref{sded} now becomes
\begin{equation}
dX_t = u(X_t)dt  + a(X_t) dB_t
\end{equation}
where $\langle [dB_t]^2\rangle  = dt$, further more we find from above that over any time interval $t$,
\begin{equation}
\int_0^t [dB_s]^2 = \langle \int_0^t [dB_s]^2\rangle = t,
\end{equation}
differentiating this with respect to $t$ (completely non-rigorously) then gives
\begin{equation}
\boxed{
[dB_t]^2 = dt.}\label{ito}
\end{equation}
Using this we can see how an arbitrary function $f$ of the process $X_t$ evolves, i.e. find the SDE for $f(X_t)$
\begin{equation}
df(X_t) = {\partial f(X_t)\over \partial x} dX_t + {1\over 2} {\partial^2 f(X_t)\over \partial x^2}
[dX_t]^2 + O(dt^{3\over 2}),
\end{equation}
here we need to keep the second term in the Taylor expansion to makes sure we have all the terms $O(dt)$. We have that
\begin{equation}
dX_t^2 = a^2(X_t) [dB_t]^2 + O(dt^{3\over 2})= a^2(X_t) dt, 
\end{equation}
where we have used Eq. \eqref{ito}. The SDE for $f$ is then
\begin{equation}
\boxed{
df(X_t) = {\partial f(X_t)\over \partial x}\left(u(X_t) dt + a(X_t) dB_t\right) + {1\over 2} {\partial^2 f(X_t)\over \partial x^2}a^2(X_t) dt .} \label{gen1}
\end{equation}
Because $dB_t$ is chosen independently of the position $X_t$, taking the average of Eq. \eqref{gen1} (and dividing by $dt$) yields
\begin{equation}
\langle {d f(X_t)\over dt} \rangle = \langle {\partial f(X_t)\over \partial x}u(X_t)  + {1\over 2} {\partial^2 f(X_t)\over \partial x^2}a^2(X_t) \rangle
\end{equation}
\section{Examples of Stochastic Differential Equations}

What a physicist calls Brownian motion is actually the process obeying the equation
\begin{equation}
mdV_t = -\gamma V_t dt + a dB_t,\label{ou}
\end{equation}
here $V_t$ is the velocity of a particle of mass $m$, the SDE is simply Newton's second law relating the momentum change on the right hand side to the force on the left hand side.
The process $V_t$, the velocity of the Brownian particle, is called an Ornstein-Uhlenbeck process.  The forces acting are: a friction term, with friction coefficient $\gamma$ plus a random noise due to molecular collisions with the solvent molecules (which are also responsible for the friction). For a spherical colloidal particle in the limit of Stokes flow (low
Reynolds number or viscous flow) the friction coefficient is given by the Stoke's formula
$\gamma = 6\pi \eta R$, where $R$ is the colloid radius and $\eta$ the liquid's viscosity.

Taking the average value of this equation shows that the average value of the velocity $\langle V_t\rangle $ is zero. Consider now the variable $S_t= V_t^2$, we see that $S_t$ obeys
\begin{eqnarray}
dS_t &=& 2V_t dV_t + {1\over 2} 2 [dV_t]^2 = 2V_t(-\frac{\gamma}{m} V_t dt + \frac{a}{m} dB_t) + \frac{a^2}{m^2} dt\nonumber \\
&=& -2 \frac{\gamma}{m} S_t dt + 2\frac{a}{m}\sqrt{S_t} dB_t + \frac{a^2}{m^2} dt.
 \end{eqnarray} 
 Taking the average of this equation then gives
 \begin{equation}
 \langle dS_t\rangle  =[ -2\frac{\gamma}{m} \langle S_t\rangle   + \frac{a^2}{m^2} ] dt.
 \end{equation}
 In thermodynamic equilibrium averages (of quantities at a single time) become time independent, and this implies that
 \begin{equation}
 \langle S_t\rangle = \frac{a^2}{2\gamma m}.
 \end{equation}
 However equipartition of energy also tells us that
 \begin{equation}
 \langle{1\over 2}m V_t^2\rangle = {1\over 2}k_BT \implies {1\over 2}m\langle S\rangle = \frac{a^2}{4\gamma},
 \end{equation}
this means the amplitude of the noise must be given by
\begin{equation}
a= \sqrt{2k_BT\gamma}.
\end{equation}
This is an example of a fluctuation dissipation theorem that relates the dissipation, friction, in a system to the fluctuations - the noise. The position of the Brownian particle, if started from the origin, is given by
\begin{equation}
X_t = \int_0^t V_s ds,
\end{equation}
the MSD is then
\begin{equation}
\langle X^2_t\rangle = \int_0^t \int_0^t ds ds' \langle V_sV_s'\rangle,
\end{equation}
so we need to know the correlation function of the velocity. We start by writing Eq. \eqref{ou} as
\begin{equation}
d[V\exp(\frac{\gamma}{m} t)] = \frac{a}{m}\exp(\frac{\gamma}{m} t) dB_t,
\end{equation}
which  we can integrate to give
\begin{equation}
V_t = V_0\exp(-\frac{\gamma}{m} t) + \frac{a}{m}\int_0^t ds \exp\left(-\frac{\gamma}{m}(t-s)\right)dB_s.
\end{equation}
Multiplying this equation by $V_0$ and taking the average gives
\begin{equation}
\langle V_t V_0\rangle  = \langle V^2_0\exp(-\frac{\gamma}{m} t) \rangle + \frac{a}{m}\langle V_0\rangle \langle \int_0^t ds \exp\left(-\frac{\gamma}{m}(t-s)\right)dB_s\rangle
\end{equation}
as the increments $dB_s$ for $s>0$ are independent of $V_0$. The last term above clearly
now has average zero as $\langle V_0\rangle =0$ and  $\langle dB_s\rangle =0$.
\begin{equation}
\langle V_t V_0\rangle  = \langle V^2_0\rangle  \exp(-\frac{\gamma}{m} t),
\end{equation}
and if we assume that $V_0$ has the equilibrium steady state distribution we find
\begin{equation}
\langle V_t V_0\rangle  = \frac{k_BT}{m} \exp(-\frac{\gamma}{m} t).
\end{equation}
Using time translational invariance of the equilibrium state we thus have $\langle V_sV_{s'}\rangle =  \frac{k_BT}{m} \exp(-\frac{\gamma}{m} |s-s'|)$ (so $V$ is an example of a coloured noise). The term $\tau = m/\gamma$ 
is a relaxation time or persistence time for the velocity. This means that the velocity is more or less the same for the time $\tau$ and changes at times bigger than $\tau$. The MSD is now given by (splitting the integral into the regions where $s<s'$ and $s>s'$ and using symmetry)
\begin{eqnarray}
\langle X^2_t\rangle  &=& 2 \frac{k_BT}{m}\int_0^t ds \int_0^s ds'  \exp\left(-\frac{\gamma}{m} (s-s')\right)= 2 \frac{k_BT}{\gamma}[\int_0^t ds [1- \exp(-\frac{\gamma}{m} s) ]\nonumber \\
&=& 2 \frac{k_BT}{\gamma} \left(t - \frac{m}{\gamma}[1-\exp(-\frac{\gamma}{m} t)]\right).
\end{eqnarray}
For short times we find that
\begin{equation}
\langle X^2_t\rangle = {k_BT \over m}t^2,
\end{equation}
this is the ballistic regime where the velocity is constant and has the Maxwell-Boltzmann
distribution. At later times however we find
 \begin{equation}
\langle X^2_t\rangle  = 2 \frac{k_BT}{\gamma} t,
\end{equation}
this means that the effective diffusion constant of the particle is given by
\begin{equation}
D = \frac{k_BT}{\gamma} 
\end{equation}
this is the famous Stokes-Einstein relation between the friction coefficient and diffusion constant. Notice that the correlation between the velocity  can be written as
\begin{equation}
\langle V_sV_{s'}\rangle  =  \frac{k_BT}{\gamma\tau} \exp(-\frac{1}{\tau} |s-s'|),
\end{equation}
where inertia is small, {\em i.e.} $m\to 0$ we have $\tau\to 0$ and
\begin{equation}
g(s-s')= \frac{1}{\tau} \exp(-\frac{1}{\tau} |s-s'|) \to 2\delta(s-s').
\end{equation}
The limit to the Dirac delta function above is clear as we notice that $g(s)$ decays over a time  $\tau$ to zero but the integral
\begin{equation}
I = \int_{-\infty}^\infty g(s) ds = \frac{1}{\tau}\int_{-\infty}^\infty ds\ \exp(-\frac{1}{\tau} |s|)
= \frac{2}{\tau}\int_0^\infty ds\ \exp(-\frac{1}{\tau} s) = 2.
\end{equation} 
 In this limit we find the mathematical version of Brownian motion. This limit is called the over damped limit where the inertial or acceleration term in Eq. \eqref{ou} can be neglected to give simply
\begin{equation}
-\gamma V_t dt + \sqrt{2k_BT \gamma}dB_t = 0 \implies V_t dt = dX_t =\sqrt{\frac{2k_BT}{\gamma}} dB_t.
\end{equation}

Another example of a stochastic differential equation is geometric Brownian motion which is used to model stock prices. The basic equation is
\begin{equation}
dX_t = \mu X_t dB_t \label{gbm}
\end{equation}
We note the Ito rule
\begin{equation}
d\ln(X_t) =\frac{dX_t}{X_t}-\frac{dX_t^2}{2 X_t^2}= \frac{dX_t}{X_t}-\mu^2\frac{dt}{2}
\end{equation}
using this in Eq. \eqref{gbm} we find
\begin{equation}
d\ln(X_t)+ \mu^2\frac{dt}{2} = \mu dB_t,
\end{equation}
which we can integrate to give
\begin{equation}
\ln(X_t)-\ln(X_0) = \mu B_t - \mu^2\frac{t}{2},
\end{equation}
we thus have the solution
\begin{equation}
X_t = X_0\exp(\mu B_t- \mu^2\frac{t}{2}).
\end{equation}

\section{The Generator and the Forward Fokker-Planck Equation}
Consider the  average value of an arbitrary function $f$ evaluated at the point $X_t$ for a stochastic process obeying an Ito SDE, started at $x_0$ at time $t=0$, by definition
\begin{equation}
\langle f(X_t)\rangle_{x_0}= \int dx \ p(x,x_0;t) f(x),
\end{equation}
the subscript $x_0$ is just to remind us that the process starts at $X_0=x_0$, this means that the initial condition is
\begin{equation}
p(x,x_0;0) =\delta(x-x_0).
\end{equation}
Now take the time derivative of the above to get
\begin{eqnarray}
\langle  \frac{d f(X_t)}{d t}\rangle_{x_0}&=& \int dx {\partial p(x,x_0;t)\over \partial t} f(x) =
 \langle {\partial f(X_t)\over \partial x}u(X_t)  + {1\over 2} {\partial^2 f(X_t)\over \partial x^2}a^2(X_t) \rangle_{x_0}\nonumber \\
&=& \int dx \ p(x,x_0;t) G f(x),
\end{eqnarray}
where the operator $G$ is called the generator and is defined by its action on a function $f$ by
\begin{equation}
Gf = \frac{1}{2}a^2(x) \frac{\partial^2f(x)}{\partial x^2} + u(x)  \frac{\partial f(x)}{\partial x}.
\end{equation}
This means that
\begin{equation}
\int dx {\partial p(x,x_0;t)\over \partial t} f(x) = \int dx \ p(x,x_0;t) Gf(x).
\end{equation}
Using the definition of  the adjoint of $G$, denoted by $G^\dagger$, we can write that that
\begin{equation}
\int dx {\partial p(x,x_0;t)\over \partial t} f(x)= \int dx \ G^\dagger p(x,x_0;t) f(x).
\end{equation}
The above is true for any reasonable functions $f$ and so we must have
\begin{equation}
{\partial p(x,x_0;t)\over \partial t} = G^\dagger p(x,x_0;t).\label{eqfp1}
\end{equation} 
The equation (\ref{eqfp1}) is called the Forward-Fokker-Planck equation, it is called a forward equation because the partial differential equation is in terms of the variable $x$, the point at which the process ends up at time $t$.  In one the dimensional systems studied here the generic Forward-Fokker-Planck equation is thus
\begin{equation}
\boxed{
{\partial p(x,x_0;t)\over \partial t} = \frac{\partial ^2}{\partial x^2}[ \frac{a^2(x)}{2} p(x,x_0;t)]
- \frac{\partial}{\partial x}[ u(x) p(x,x_0;t)]}.
\end{equation}
When $a$ and $u$ are independent of time he formal solution to the Fokker-Planck equation can be written as
\begin{equation}
p(x,x_0;t) =\exp(tG^\dagger) \delta(x-x_0),
\end{equation}
where the operator $G^\dagger$ acts on the variable $x$, this is
analogous to the solution of the Schr\"odinger equation.

\section{Generalisation to higher dimensions}
The generalisation of Ito Calculus to higher dimensions is a straightforward exercise. The form of the SDE is
\begin{equation}
dX_i = u_i({\bf X}) dt+ a_{ij}({\bf X}) dB_j \label{itogen}
\end{equation}
here the $dB_i$ are independent stochastic increments for each spatial direction $i$. Here the  Ito rule is $dB_idB_j = \langle dB_i dB_j\rangle = \delta_{ij}dt$. Recalling that the second order Taylor expansion in $d$ dimensions (using the Einstein summation convention) is given by
\begin{equation}
f({\bf x} + {\bf y}) = f({\bf x}) + y_i {\partial \over \partial x_i}f({\bf x}) + \frac{1}{2}
y_i y_j \frac{\partial^2 f({\bf x})}{\partial x_i\partial x_j} +O({\bf y}^3).
\end{equation}
we see that generator is given by
\begin{equation}
Gf = \frac{1}{2}a_{ik}({\bf x})a_{jk}({\bf x}) \frac{\partial^2 f({\bf x})}{\partial x_i\partial x_j}
+ u_i({\bf x}) {\partial \over \partial x_i}f({\bf x}).
\end{equation}
The corresponding Forward-Fokker-Planck equation is thus
\begin{equation}
\boxed{
{\partial p({\bf x},{\bf x}_0;t)\over \partial t} = G^\dagger p({\bf x},{\bf x}_0;t)=\frac{\partial ^2}{\partial x_i x_j}[ \frac{a_{ik}(x)a_{jk}(x)}{2} p({\bf x},{\bf x}_0;t)]
- \frac{\partial}{\partial x_i}[ u_i({\bf x}) p({\bf x},{\bf x}_0;t)].}
\end{equation}
Here the formal solution can be written as
\begin{equation}
p({\bf x},{\bf x}_0;t) =\exp(tG^\dagger) \delta({\bf x}-{\bf x}_0),\label{schrod}
\end{equation}


\section{A brief word on other forms of Stochastic Calculus.} 
There are other formulations of the stochastic calculus that differ from the Ito calculus in that the increment $dB_t$ is correlated with the current particle position $X_t$. The most 
popular one is the Stratonovich version  which looks like normal calculus without the second order derivative. Another version is the ant-Ito calculus, where some natural physical processes take their most simple form. However if we fix the Fokker-Planck equation the corresponding Ito, Stratonovich and anti-Ito processes will have the same statistics - only the stochastic equations will appear different. If the noise term does not depend on the position $X_t$, {\em i.e.} $a_{ij}$ is constant, the process is independent of
the version of the calculus used.
\section{Links with physical descriptions of diffusion}
In physics diffusion equations are often derived in terms of particle concentrations $c({\bf x};t)$, for particles that do not interact the concentration of particles evolves like the probability density function of a single particle and is only mathematically different in that  it is not normalised to unity. In equilibrium, in the absence of external forces, tracer particles in a liquid or solid should have a uniform density which maximises the entropy of the system. 
If the distribution is not uniform the system will relax towards equilibrium, particles move from regions where they are concentrated to regions where they are scarce. This means that a gradient of particle concentration should lead to a flow or current to redistribute the particles. Fick's first law states that
\begin{equation}
\boxed{
{\bf J} = -D \nabla c,}
\end{equation}
where $D$ is the local collective diffusion constant which can depend on the environment.
Along with the conservation equation 
\begin{equation}
{\partial c({\bf x},t)\over \partial t} + \nabla \cdot {\bf J} =0, 
\end{equation}
we find the diffusion equation
\begin{equation}
{\partial c({\bf x};t)\over \partial t} =\nabla\cdot D({\bf x})\nabla c({\bf x};t).
\end{equation}
From this we see that the generator for the corresponding SDE can be deduced by
noting that
\begin{equation}
G^\dagger f =  \nabla\cdot D({\bf x})\nabla f \implies G f =  \nabla\cdot D({\bf x})\nabla f = D({\bf x})\nabla^2 f + \nabla D({\bf x})\cdot\nabla f
\end{equation}
so here $G=G^\dagger$ so $G$ is self adjoint. From $G$ we can read off the Ito SDE corresponding to the Fick diffusion equation which is
\begin{equation}
 d{\bf X}_t = \sqrt{2 D({\bf X}_t)} d{\bf B}_t + \nabla D({\bf X}_t) dt.\label{sdedif}
\end{equation}
This somewhat surprising result shows that, interpreted as an Ito process, a pure diffusion has a local drift or bias forcing it into regions where $D$ is large ! Without this drift term
the steady state distribution in a finite volume $V$ is given by
\begin{equation}
p_{s}({\bf x}) \propto \frac{1}{D({\bf x})},
\end{equation}
rather than a uniform distribution. Physically, in regions where $D$ is large the particle diffuses quickly and thus leaves them quickly, while it spends more time in regions where it diffuses slowly. The drift term is exactly the term necessary to counter this tendency by pushing particles back into the region of high diffusivity.  

One way of obtaining a fluctuating diffusivity is by changing the temperature in the solvent or gas in which the particles diffuse. The Stokes-Einstein formula for the diffusivity is
\begin{equation}
D= \frac{k_BT}{\gamma} = \frac{k_BT}{6\pi R \eta(T)},
\end{equation}
so there is a dependency on the temperature $T$ that can be very strong, especially for the 
viscosity $\eta$ that can vary strongly with temperature, for example close to a liquid-glass transition. On top of this there is the effect of thermophoresis. In equilibrium the temperature should be uniform and so a gradient in temperature can generate a current. This is the so called Soret effect where particles move along a temperature gradient. The current associated with a temperature gradient is given by
\begin{equation}
{\bf J}_T = -D_T c\nabla T,
\end{equation}
its strength depends on the temperature gradient but also the local concentration of the particle number, the more particles there are the stronger the current. The factor $D_T$ depends on the precise details of the interaction between the particles and the solvent. For example, smoke particles in a gas move away from regions of high temperature and $D_T$ is thus positive. Physically this happens because, for a particle of finite size, the molecules on the hot side transfer more momentum than the molecules on the cooler side, thus pushing the particle away from regions of high temperature.  For liquid solvents however,  $D_T$ can be negative.

If we write the overall 
diffusion equation including the currents coming from the concentration gradient and the temperature gradient we find
\begin{equation}
{\partial c({\bf x};t)\over \partial t} = \nabla \cdot\left(D({\bf x})\nabla c({\bf x},t) + D_T({\bf x}) c({\bf x},t) \nabla T({\bf x})\right).
\end{equation}

Consider now a particle subject to an external force field in a solvent which exerts a random white noise force on the particle. The SDE for the velocity is
\begin{equation}
m d{\bf V}_t = -\gamma {\bf V}  dt + {\bf F}({\bf X}_t) dt + \sqrt{2 k_BT\gamma} d{\bf B}_t .
\end{equation}
In the over damped limit, $m\to 0$, this simplifies to give
\begin{equation}
d{\bf X}_t = {1\over \gamma}{\bf F}({\bf X}_t) dt + \sqrt{\frac{2 k_BT}{\gamma}} d{\bf B}_t.
\end{equation}
Often this sort of equation is written in terms of the bare diffusivity that the particle
would have in the absence of the force, {\em i.e.} $ D = k_BT/\gamma$ and so we have
\begin{equation}
d{\bf X}_t = \beta D{\bf F}({\bf X_t}) dt + \sqrt{2D} d{\bf B}_t.
\end{equation}
The generator $G$ is given by
\begin{equation}
Gf = D\nabla^2 f + \beta D{\bf F}\cdot\nabla f,
\end{equation}
where $\beta = 1/k_BT$, and from this we find the Fokker-Planck equation
\begin{equation}
\frac{\partial p({\bf x},{\bf x}_0,t)}{\partial t} = \nabla \cdot (D\nabla p({\bf x},{\bf x}_0,t) -D\beta {\bf F}({\bf x}) p({\bf x},{\bf x}_0,t)).
\end{equation}
In the case where ${\bf F}$ is conservative, {\em i.e.} generated by a potential energy $\phi$ 
such that ${\bf F} = -\nabla \phi$ we see that the steady state distribution is given by the Gibbs Boltzmann distribution for the canonical ensemble
\begin{equation}
p_{eq}({\bf x}) = \frac{\exp\left(-\beta \phi({\bf x})\right)}{Z},
\end{equation}
where 
\begin{equation}
Z = \int_V d{\bf x}\ \exp\left(-\beta \phi({\bf x})\right),
\end{equation}
is the canonical partition function which ensures the normalisation of the probability density function over the finite volume $V$ of the system. The solvent generating the stochastic noise exchanges energy with the particle and acts as the reservoir in the canonical ensemble. 
  
\section{First passage times}
Imagine we have a problem where a molecule or colloid whose dynamics can be described by a SDE reacts when it hits a certain surface, given its starting point ${\bf x}$ we can ask what is the average value of the time at which the particle arrives at the reactive 
surface? Denote by $T$ the first passage time (FPT) to the surface $\Sigma$ and define its average value, the mean first passage time (MFPT), starting from ${\bf x}$ by $t({\bf x}) = \langle T\rangle_{x})$. There are physical arguments based on fluxes to derive  MFPTs but let's find the general equation probabilistically. If the particle starts at ${\bf x}$ at time $t=0$, in the following time interval $dt$ the particle will move to ${\bf x}+ d{\bf X}$ but also the time elapsed will increase by $dt$, this means that
\begin{equation}
t({\bf x}) = \langle  t({\bf x}+d{\bf X}))\rangle + dt ,
\end{equation}
where the average $\langle \cdot\rangle$ is over the first step $dX$ taken by the process from the starting position ${\bf x}$.
Now expanding $t({\bf x}+d{\bf X})$ to second order in $d{\bf X}$ gives
\begin{equation}
t({\bf x}) = t({\bf x})+ Gt({\bf x}) dt + dt,
\end{equation}
the $O(1)$ terms cancel and the remaining terms of $O(dt)$ then give
\begin{equation}
\boxed{
Gt({\bf x}) = -1.}
\end{equation}
The only thing we need now are the boundary conditions, however if one starts on $\Sigma$ then the first passage time to $\Sigma$ is zero and so $t({\bf x}) =0$ on $\Sigma$ are the boundary conditions. Consider a BM with diffusion constant $D$ in one dimension started at some point $x\in [a,b]$, what is the expected hitting time of the edge of the interval? This means that 
\begin{equation}
D\frac{\partial^2}{\partial x^2} t(x) = -1
\end{equation}
which has general solution $ t(x) = -x^2/2D + cx +d$, the boundary conditions then allow the determination of the constants of integration $c$ and $d$ to yield
\begin{equation}
t(x) = \frac{(b-x)(x-a)}{2D}.
\end{equation}
This result is interesting, it means that if $b$ becomes large then the MPFT scales like $b$ even if we start close to $a$. Most particles starting close to $a$ will hit $a$ within a finite time, however a few will go the wrong way and take a very long time to come back !

 Another question that helps elucidate the MFPT result above is whether the particle in question hits the point A or the point B first. One can imagine a problem with reactants in a two dimensional channel that stick to the upper or lower surfaces, from an initial concentration profile what proportion of the reactants get stuck to each surface? Denote by $P_a(x)$ the probability that the particle hits the point $a$ before the point $b$. Again we just look at what happens in the first time step $dt$ and write (consider a 1d problem - the generalisation to higher dimensions is straightforward)
\begin{equation}
P_a(x) = \langle P_a(x+ dX)\rangle_{\bf x} = P_a(x) + GP_a(x) dt,
\end{equation}
so we simply find $GP_a(x) = 0$. The boundary conditions are obviously $P_a(a)=1$ and $P_a(b)=0$. The solution is thus $P_a(x) = (b-x)/(b-a)$ 

Now consider a particle diffusing in a potential in one dimension which is periodic with 
period $L$ as shown in Fig. (\ref{1dpot}). The MFPT to points $a$ and $b$ starting from $x$ obeys
\begin{equation}
D{d^2\over d x^2} t(x) - D\beta \frac{d}{d x}\phi(x) \frac{d}{d x}t(x) =-1,
\end{equation}
and this equation can be integrated to give
\begin{equation}
t(x) = -\frac{1}{D}\int_a^xdx'\ \exp\left(\beta\phi(x')\right)\int^{x'}_a dy\ \exp\left(-\beta
\phi(y)\right) + A\int_a^x dx' \exp\left(\beta\phi(x')\right),
\end{equation}
where in doing the last integration we have used $t(a)=0$. The condition $t(b)=0$ then gives the constant $A$ and we obtain
\begin{eqnarray}
t(x) &=& -\frac{1}{D}\int_a^xdx'\ \exp\left(\beta\phi(x')\right)\int^{x'}_a dy\ \exp\left(-\beta
\phi(y)\right) \nonumber \\ &+& \left[ \frac{1}{D} \frac{\int_a^bdz\ \exp\left(\beta\phi(z)\right)\int^{z}_a dy\ \exp\left(-\beta
\phi(y)\right)}{\int_a^b dz\exp\left(\beta\phi(z)\right)}\right]\int_a^x dx' \exp\left(\beta\phi(x')\right).
\end{eqnarray}
Now define by $t(x;L)$ the MFPT to move one period $L$ to the left or the right of $x$, this
means just choosing  $a=x+L$ and $b= x-L$. \begin{eqnarray}
t(x;L) &=& -\frac{1}{D}\int_{x-L}^{x}dx'\ \exp\left(\beta\phi(x')\right)\int^{x'}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right) \nonumber \\ &+& \left[ \frac{1}{D} \frac{\int_{x-L}^{x+L}dz\ \exp\left(\beta\phi(z)\right)\int^{z}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right)}{\int_{x-L}^{x+L} dz\exp\left(\beta\phi(z)\right)}\right]\int_{x-L}^x dx' \exp\left(\beta\phi(x')\right).\nonumber \\
\end{eqnarray}
Due to the periodicity of $\phi(x)$, $\phi(x+n L) = \phi(x)$ for integer $n$, a number of the 
integrals become independent of $x$ and we get
\begin{eqnarray}
t(x;L) &=& -\frac{1}{D}\int_{x-L}^{x}dx'\ \exp\left(\beta\phi(x')\right)\int^{x'}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right) \nonumber \\ &+&  \frac{1}{2D} \int_{x-L}^{x+L}dz\ \exp\left(\beta\phi(z)\right)\int^{z}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right)
\end{eqnarray}
which partially simplifies to
\begin{eqnarray}
t(x;L) &=& -\frac{1}{2D}\int_{x-L}^{x}dx'\ \exp\left(\beta\phi(x')\right)\int^{x'}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right) \nonumber \\ &+&  \frac{1}{2D} \int_{x}^{x+L}dz\ \exp\left(\beta\phi(z)\right)\int^{z}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right),\end{eqnarray}
now changing variables in the $x'=z-L$ in the first integral and using periodicity gives
\begin{eqnarray}
t(x;L) &=& \frac{1}{2D} \int_{x}^{x+L}dz\ \exp\left(\beta\phi(z)\right)\left[\int^{z}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right) -\int^{z-L}_{x-L} dy\ \exp\left(-\beta
\phi(y)\right)\right] \nonumber \\
&=& \frac{1}{2D} \int_{x}^{x+L}dz\ \exp\left(\beta\phi(z)\right)\int^{z}_{z-L} dy\ \exp\left(-\beta
\phi(y)\right) \nonumber \\
&=& \frac{1}{2D} \int_{0}^{L}dz\ \exp\left(\beta\phi(z)\right)\int^{L}_{0} dz\ \exp\left(-\beta
\phi(z)\right) \label{mfpt}
\end{eqnarray}
remarkably we see
that $t(x;L) = t(L)$ is independent of $x$. It is useful to visualise the coarse grained diffusion
as a discrete random walk on the points $nL$ - we say the discrete process is at the site
$nL$ until the continuous diffusion next arrives at $(n-1)L$ or $(n+1)L$, at which point the discrete process changes. This discrete random walk has no bias, it is equally likely to go to the left or right - we can see this by computing the probability that starting at $x$ it hits $x+L$ before $x-L$. This probability of a positive step $P_+(x)$ (hitting $x+L$ before $x-L$ obeys
\begin{equation}
D{d^2\over d x^2} P_+(x) - D\beta \frac{d}{d x}\phi(x) \frac{d}{d x}P_+(x) =0.
\end{equation}
The general solution can be written as
\begin{equation}
P_+(x)= A\int_a^x dx' \exp\left(\beta\phi(x')\right)
\end{equation}
where here  $a= x-L$ is the target to the left. Of course $P_+(x+L) =1$ so we find
\begin{equation}
P_+(x)= \frac{\int_{x-L}^x dx' \exp\left(\beta\phi(x')\right)}{\int_{x-L}^{x+L} dx' \exp\left(\beta\phi(x')\right)},
\end{equation}
the periodicity of $\phi$ then gives The general solution can be written as
\begin{equation}
P_+(x)= A\int_a^x dx' \exp\left(\beta\phi(x')\right)
\end{equation}
where here  $a= x-L$ is the target to the left. Of course $P_+(x+L) =1$ so we find
\begin{equation}
P_+(x)= \frac{\int_{x-L}^x dx' \exp\left(\beta\phi(x')\right)}{\int_{x-L}^{x+L} dx' \exp\left(\beta\phi(x')\right)},
\end{equation}
the periodicity of $\phi$ then gives $P_+(x) = 1/2$,where we have repeatedly used the periodicity of the potential.

The fact that $P_+(x) = 1/2$, means that the discrete random walk so constructed is unbiased. Physically this has to be true but the maths is reassuring!
We can actually use the mean first passage time distribution to compute the effective diffusion constant in this system. If the discrete random walk takes $N$ steps during the time $t$ is has a MSD (started from the origin)
\begin{equation}
\langle X^2_t\rangle  = L^2 N.
\end{equation}
However $N\times t(L) = t$, the number of jumps made times the average time to jump
must equal the total time (at late times) and so
\begin{equation}
\langle X^2_t\rangle  = L^2 \frac{t}{t(L)} = 2D_e t,
\end{equation}
thus the effective diffusion constant $D_e$ is given by
\begin{equation}
\boxed{
D_e =\frac{L^2}{2t(L)} = \frac{D}{\frac{1}{L} \int_{0}^{L}dz\ \exp\left(\beta\phi(z)\right)\frac{1}{L} \int_{0}^{L}dz\ \exp\left(-\beta\phi(z)\right)}.}
\end{equation}
This result can be written in terms of the spatial averages
\begin{equation}
\langle \exp\left(\pm \beta\phi(z)\right)\rangle_s = {1\over L}\int_{0}^{L}dz\ \exp\left(\pm\beta\phi(z)\right),
\end{equation}
to give
\begin{equation}
D_e =\frac{D}{\langle \exp\left( \beta\phi\right)\rangle_s \langle \exp\left( -\beta\phi\right)\rangle_s}.\label{1dpotf}
\end{equation}
Using Jensen's inequality one can show that $D_e < D$ so the diffusion is alway slowed
down by the presence of a potential  - this is due to the trapping in the minima of the potential. It is also amusing to note that the effective diffusion constant in the potential $\phi$ is identical to that in the inverted potential $-\phi$. 

Using the expression for the MPFT Eq. \eqref{mfpt} we can analyse what happens at low temperature. This means that $\beta$ is large and so the integral
\begin{equation}
I_- = \int_0^L  dz \exp\left(-\beta \phi(z)\right)
\end{equation}
can be approximated about its minimal value occurring at $z_{\rm min}$ (assuming it is unique). To do the integral we write $z = z_{\rm min} + \zeta$ and Taylor expand about $z_{\rm min}$ to get
\begin{eqnarray}
I_- &\approx& \int_0^L  d\zeta \exp\left(-\beta \phi(z_{\rm min}) -\frac{\beta}{2} \phi''(z_{\rm min})\zeta^2\right) \nonumber \\
&\approx& \int_{-\infty}^{\infty}  d\zeta \exp\left(-\beta \phi(z_{\rm min}) -\frac{\beta}{2} \phi''(z_{\rm min})\zeta^2\right) =\sqrt{\frac{2\pi}{\beta \phi''(z_{\rm min})} }\exp\left(-\beta \phi(z_{\rm min})\right).\nonumber \\
\end{eqnarray}
The same method of calculation estimates $I_+$ from the contribution of the maximum of
$\phi$ at the point $z_{\rm max}$ and gives
\begin{equation}
I_+ \approx \sqrt{\frac{2\pi}{\beta |\phi''(z_{\rm max})|} }\exp\left(\beta \phi(z_{\rm max})\right).
\end{equation}
We thus obtain the {\em Arrhenius law} for thermally activated energy barrier crossing
\begin{equation}
\boxed{
t(L) \approx \tau_0 \exp(\beta \Delta \phi),}
\end{equation} 
where $\Delta \phi = \phi(z_{\rm max}) - \phi(z_{\rm min})$ is the biggest energy barrier the 
particle has to cross. The factor $\tau_0$ is a time scale given by
\begin{equation}
\tau_0 =\frac{2\pi}{D\beta}\frac{1}{\sqrt{ \phi''(z_{\rm min})|\phi''(z_{\rm max})|}} ,
\end{equation}
we see that it  depends on the details of the dynamics, the temperature, and the local curvature of the potential at the absolute maximum and minimum. At low temperatures the particle spends all its time trying to cross the barrier and the result becomes essentially independent of $L$.
\section{The Langevin equation}
In the physics literature it is common to write stochastic equations in the form
\begin{equation}
\boxed{
\frac{dX}{dt} = u(X_t) + a(X_t)\eta(t)}
\end{equation}
where we have 
\begin{equation}
\eta(t) = {dB_t\over dt},
\end{equation}
clearly $\langle \eta(t)\rangle = 0$.
In order to determine the correlation function of $\eta(t)$ defined as
\begin{equation}
c(t,t') =\langle \eta(t)\eta(t')\rangle,
\end{equation}
consider the integral 
\begin{equation}
I_f = \int dt \ \eta(t) f(t)
\end{equation}
for an arbitrary function $f$. To start with, assuming $f$ is independent of $\eta$, one has
 $\langle I_f\rangle = 0$, however 
 \begin{equation}
 \langle I_f I_g\rangle  = \iint dt dt' f(t) g(t') c(t,t'),\label{wn1}
\end{equation} 
where $g$ is another arbitrary function of $t$. Now we compute the same correlation function using the Ito calculus where
\begin{equation}
I_f = \int dB_t f(t),
\end{equation}
from this we find
\begin{equation}
\langle I_f I_g\rangle = \int dt f(t) g(t) = \int dt dt' f(t) g(t')\delta(t-t')\label{wn2}.
\end{equation}
and so we have that $c(t,t') = \delta(t-t')$. The process $\eta(t)$ is called Gaussian white noise. Stochastic equations written in terms of white noise are called Langevin equations.

\section{Stochastic Processes in Fourier space}
Often it is useful to analyse stochastic processes in Fourier space, this is particularly useful
when the correlation function is invariant under time translation, that is to say
\begin{equation}
\langle X(t)X(t')\rangle = C(t,t')=C(t-t')
\end{equation}
In particular this most hold in equilibrium when the statistical properties of the system do not evolve with time. To see this, we note that  the system at time $\tau$ has the same statistics at the time $0$ and so we must have
\begin{equation}
\langle X(t)X(t')\rangle = \langle X(t+\tau)X(t'+\tau)\rangle,
\end{equation}
and so
\begin{equation}
C(t,t') = C(t+\tau, t'+\tau),
\end{equation}
 which can only be satisfied if $C(t,t')=C(t-t')$. 
 
 The Fourier transform of $X(t)$ is defined by (note there is a sign difference in this convention to that usually employed in physics)
 \begin{equation}
 \tilde X(\omega) = \int_{-\infty}^{\infty} dt \exp(i\omega t) X(t)
 \end{equation}
 and its correlation function at two Fourier frequencies $\omega$ and $\omega'$ is given
 by
 \begin{eqnarray}
 \langle \tilde X(\omega)\tilde X(\omega')\rangle &=& \int_{-\infty}^{\infty} dt dt'\ 
 \exp(i\omega t +i\omega' t') \langle X(t)X(t')\rangle\nonumber \\
 &=&\int_{-\infty}^{\infty} dt dt'\ 
 \exp(+i\omega t +i\omega' t') C(t-t') \ {\rm write}\ t'=t-\tau \nonumber \\
 &=&\int_{-\infty}^{\infty} dt d\tau\ 
 \exp(i[\omega  +\omega' ]t -i\omega'\tau) C(\tau).
 \end{eqnarray}
 However we use the Fourier transform identity 
 \begin{equation}
 \int_{-\infty}^\infty dt\ \exp(i\omega t)= 2\pi \delta(\omega)
 \end{equation}
 to find
 \begin{equation}
 \langle \tilde X(\omega)\tilde X(\omega')\rangle = 2\pi \delta(\omega+\omega')\tilde C(-\omega') = 2\pi \delta(\omega+\omega')\tilde C(\omega).
 \end{equation}
 The above relation is often written using $\tilde X(-\omega)= \tilde X^*(\omega)$ (where $^*$ denotes the complex conjugate) if $X$ is real, as
 \begin{equation}\boxed{
 \langle \tilde X(\omega)\tilde X^*(\omega')\rangle  = 2\pi \delta(\omega- \omega')S(\omega),}
 \end{equation}
 where 
 \begin{equation}
 S(\omega) = \tilde C(\omega)
 \end{equation}
 is called the spectral density of the process or signal $X$. 
 \section{The partially damped simple Harmonic oscillator via the Langevin equation}
 Consider a Brownian particle confined by a Harmonic potential $V(x) = m\Omega^2 x^2/2$ and subject to an external force which depends on time $f(t)$. The corresponding Langevin equation is given by
 \begin{equation}
 m\frac{d^2X(t)}{dt^2} = -\gamma \frac{dX(t)}{dt} -m\Omega^2 X(t)  + f(t)+\sqrt{2k_BT\gamma}\eta(t).
 \end{equation}
 Fourier transforming the Langevin equation gives
 \begin{equation}
 -m\omega^2 \tilde X(\omega) = \gamma i\omega \tilde X(\omega) - m\Omega^2 \tilde X(\omega) +\sqrt{2k_BT\gamma} + \tilde f(\omega)+\sqrt{2k_BT\gamma}\tilde\eta(\omega), 
 \end{equation}
 which gives the solution
 \begin{equation}
 \tilde X(\omega) = \frac{\tilde f(\omega)+\sqrt{2k_BT\gamma}\tilde\eta(\omega)}{m[\Omega^2-\omega^2] - i\omega\gamma}.
 \end{equation}
 To start with we take the average of this equation to obtain
 \begin{equation}
 \langle \tilde X(\omega)\rangle = \tilde\chi(\omega)\tilde f(\omega),\label{chif}
 \end{equation}
 where 
 \begin{equation}
 \tilde\chi(\omega) = \frac{1}{m[\Omega^2-\omega^2] - i\omega\gamma},
 \end{equation}
 Using the convolution theorem for Fourier transforms we can invert the relation Eq. \eqref{chif} to write 
 \begin{equation}
 \langle\tilde X(t)\rangle = \int_{-\infty}^\infty \chi(t-t')f(t')dt,
 \end{equation}
 where the {\em response function} $\chi(t-t')$ describes the response of the average value of $X(t)$ due to a force acting at time $t'$. Clearly what the force does at times greater than $t$ cannot affect the system's response at time $t$ - this is causality. This means that in any physical model $\chi(t-t')
 =0$ for $t'>t$ and so we can write
 \begin{equation}
 \langle\tilde X(t)\rangle = \int_{-\infty}^t \chi(t-t')f(t')dt.
 \end{equation}
 We also see that $\tilde \chi$ has both real and imaginary parts, we write
 $\tilde \chi = \tilde \chi'(\omega) + i\tilde \chi''(\omega)$, so where $\tilde \chi'(\omega)$ denotes the real part (the in-phase susceptibility) and $\tilde \chi''(\omega)$ the imaginary part (out-of phase susceptibility). Here we find that
 \begin{eqnarray}
 \tilde \chi'(\omega)&=& \frac{m[\Omega^2-\omega^2]}{m^2[\Omega^2-\omega^2]^2+\omega^2\gamma^2}\\
 \tilde \chi''(\omega) &=& \frac{\omega\gamma}{m^2[\Omega^2-\omega^2]^2+\omega^2\gamma^2}\label{chipp}
 \end{eqnarray}

 
 Now consider the correlation function at zero force $f$
  \begin{equation}
 \langle \tilde X(\omega)\tilde X(\omega')\rangle = 2\pi \delta(\omega+\omega')\tilde C(\omega) = 4\pi k_BT\gamma \delta(\omega+\omega')\frac{1}{m[\Omega^2-\omega^2] - i\omega\gamma}\times\frac{1}{m[\Omega^2-\omega'^2] - i\omega'\gamma},
 \end{equation}
 which gives
 \begin{equation}
 \tilde C(\omega) = \frac{2k_BT\gamma}{m^2[\Omega^2-\omega^2]^2+\omega^2\gamma^2}.
 \label{Cho}
 \end{equation}
 Now comparing Eqs. \eqref{chipp} and \eqref{Cho} we see that
 \begin{equation}
 \frac{\tilde C(\omega)}{\tilde \chi''(\omega)} = \frac{2k_BT}{\omega}
 \end{equation}
 which is usually written as
 \begin{equation}
 \boxed{
 \tilde \chi''(\omega) = \frac{\beta}{2}\omega\tilde C(\omega)}.\label{fdt1}
 \end{equation}
 This formula is a special case of the classical (non-quantum) fluctuation dissipation theorem which in general is true for {\em small applied forces} $f(t)$ (in the regime of {\em linear response}) for systems which are {\em thermal equilibrium} but for 
 any dynamics obeying detailed balance which is (we shall see later) the criterion necessary for the equilibrium dynamics to be described by equilibrium Gibbs-Boltzmann statistics. The dynamical variables $\chi$ and $C$ appearing in Eq. \eqref{fdt1} do depend on the details of
 the dynamics but the relation holds independently of the dynamics. 

\section{Kramers-Kronig Relation}  
The imaginary part of the response function $\chi$ which tells us how the system reacts when it is perturbed way from equilibrium is thus related to a purely equilibrium quantity - the correlation function - which can be possibly calculated of measured by experiments on equilibrium (non-perturbed) systems. However it is not clear how to obtain the real part of the response function can be measured. The property of causality can however be used to determine the full response function.

To begin with we know that $\chi(t) =0$ for $t<0$ by causality. This means that we can write
\begin{equation}
\tilde\chi(\omega+i\epsilon) = \int_0^\infty dt \exp(i\omega t-\epsilon(t)) \chi(t)
\end{equation}
 and we can recover $\tilde \chi(\omega)$ by taking $\epsilon \to 0$ for $\epsilon>0$ (to ensure the convergence of the integral. We notice that
 \begin{equation}
 \frac{1}{2} \int_{-\infty}^\infty dt\ \exp(i\omega t) [\chi(t) -\chi(-t)] = \frac{1}{2}[\tilde \chi(\omega)
 -\tilde \chi(-\omega)].
 \end{equation}
 However $\chi(t)$ is real so $\tilde \chi(-\omega)=\tilde\chi^*(\omega)$ so
 \begin{equation}
 \frac{1}{2} \int_{-\infty}^\infty dt\ \exp(i\omega t) [\chi(t) -\chi(-t)] = \frac{1}{2}[\tilde \chi(\omega)
 -\tilde \chi^*(\omega)] = i \tilde\chi''(\omega).
 \end{equation}
 Now inverting the Fourier transform gives for $t>0$
 \begin{equation}
 \frac{1}{2}[\chi(t) -\chi(-t)] = \frac{i}{2\pi}\int_\infty^\infty d\omega' \exp(-i\omega't) \tilde\chi''(\omega'),
 \end{equation}
  which can be used to write
 \begin{equation}
  \frac{1}{2}\int_0^\infty dt \exp(i\omega t) \chi(t) =  \frac{1}{2} \tilde\chi(\omega)
  =  \frac{i}{2\pi}\int_\infty^\infty d\omega' \int_0^\infty dt\ \exp\left(-i(\omega'-\omega)t\right) \tilde \chi''(\omega')
 \end{equation}
 Now if we write $\omega = \omega + i\epsilon$ we can write
 \begin{equation}
 \int_0^\infty dt\ \exp\left(-i(\omega'-\omega)t\right) = \int_0^\infty dt\ \exp\left(-i(\omega'-\omega)t -\epsilon t\right) = \frac{1}{\epsilon + i(\omega'-\omega)},
 \end{equation} 
  and this gives
 \begin{equation}
 \tilde \chi(\omega+i\epsilon) = \frac{1}{\pi}\int_{-\infty}^{\infty} d\omega' \ \frac{\chi''(\omega')}{\omega'-\omega - i\epsilon}
 \end{equation}
 Taking the limit $\epsilon\to 0$ then gives
 \begin{equation}
 \boxed{
 \tilde \chi(\omega) = \frac{1}{\pi}{\cal P} \int_{-\infty}^{\infty} d\omega' \frac{\chi''(\omega')}{\omega'-\omega},}
 \end{equation}
 where ${\cal P}$ denotes the Cauchy principal value.
 

 \chapter{Markov chains}
 \section{Introduction}
 A very general class of dynamics for systems with discrete phase spaces can be constructed as Markov chains. Imagine a phase space $\Omega$ which is discrete 
 (we can approximate continuous phase spaces with large enough discrete spaces).
 Let the microstate be labeled by $C$ and let the dynamics evolve in discrete time denoted
 by $n$. Let $P_n(C)$ denote the probability that at time $n$ the system is in the state
 $C$. In the next time step if the system is in the state $C$ it can jump to another state $C'$ with transition probability $\rho(C\to C')$, what is does only depends on its current state at time $n$ - this is what defines a Markov process. In the same way the evolution of a Langevin equation
in the time $[t,t+dt]$ only dependents on the current value ${\bf X}(t)$ and the evolution of a Hamiltonian dynamical system only depends on the current  microstate defined via the position ${\bf X}$ and the corresponding momenta ${\bf P}$. Consider $P_{n+1}(C)$, the probability to be in the state $C$ at the time $n+1$. This can happen if (i) the system is
in the state $C$ at time $n$ and stays there - it does this with probability $\rho(C\to C)$ (ii)
if the system is in one of the other states $C'$ and jumps to the state $C$ - it does with with probability $\rho(C\to C')$ so we find the discrete time master equation
\begin{equation}
\boxed{
P_{n+1}(C) =  \rho(C\to C)P_n(C) + \sum_{C'\neq C} \rho(C'\to C)P_n(C').}
\end{equation}
Conservation of probability (the particle must stay at its current site or go somewhere else in phase space) means that
\begin{equation}
\sum_{C'} \rho(C' \to C) = 1.\label{norm}
\end{equation}
Now if the dynamics describes a physical system interacting with a heat bath (energy reservoir), the equilibrium distribution is given by the canonical ensemble
\begin{equation}
P_{eq}(C) = \frac{\exp(-\beta E(C))}{Z},
\end{equation}
where 
\begin{equation}
Z = \sum_C \exp(-\beta E(C)),
\end{equation}
is the canonical partition function. The equilibrium distribution does not evolve with $n$ and so must obey
\begin{equation}
P_{eq}(C) =  \rho(C\to C)P_{eq}(C) + \sum_{C'\neq C} \rho(C'\to C)P_{eq}(C').
\end{equation}
and so 
\begin{equation}
\exp(-\beta E_C) =  \rho(C\to C)\exp(-\beta E(C))+ \sum_{C'\neq C} \rho(C'\to C)\exp(-\beta E(C')).
\end{equation}
If we choose the transition probabilities such that they obey the {\em detailed balance} condition
\begin{equation}
\boxed{
\frac{\rho(C'\to C)}{\rho(C \to C')} = \frac{\exp(-\beta E(C))}{\exp(-\beta E(C'))}}
\end{equation} 
we find
\begin{eqnarray}
&&\rho(C\to C)\exp(-\beta E(C))+ \sum_{C'\neq C} \rho(C'\to C)\exp(-\beta E(C'))\nonumber\\ &=&
\rho(C\to C)\exp(-\beta E(C))+\sum_{C'\neq C} \rho(C\to C')\exp(-\beta E(C))\\ \nonumber
&=& \exp(-\beta E(C)) \sum_{C'}\rho(C\to C') = \exp(-\beta E(C)),
\end{eqnarray}
where we have used Eq. \eqref{norm},
and thus the Gibbs-Boltzmann distribution is indeed the equilibrium distribution.
\section{Monte Carlo simulations}
Systems with a canonical heat bath can be simulated on a computer using an algorithm
obeying detailed balance. For example consider a system of $N$ Ising spins $S_i=\pm1$ interacting via a Hamiltonian $H(S_1, S_2,\cdots S_N)$. We choose $1$ of the spins randomly uniformly with a probability $p=1/N$ and calculate the new energy of the system when the spin, $S_j$ say is changed to $-S_j$. In Metropolis dynamics the probability of accepting the spin flip $p_a(S_j\to -S_j)$ is given by 
\begin{equation}
p_a(S_j\to -S_j)
\end{equation}
if $H(S_1, S_2,\cdots, -S_j,\cdots S_N) < H(S_1, S_2,\cdots, S_j,\cdots S_N)$ but if $H(S_1, S_2,\cdots, -S_j,\cdots S_N) > H(S_1, S_2,\cdots, S_j,\cdots S_N)$ then the flip is accepted with probability 
\begin{equation}
p_a(S_j\to -S_j) = \exp\left[ -\beta\left(H(S_1, S_2,\cdots, -S_j,\cdots S_N) - H(S_1, S_2,\cdots, S_j,\cdots S_N)\right)\right] <1.
\end{equation}
The total probability at a given discrete time of changing $S_j$ is thus equal to 
\begin{equation}
p(S_j\to -S_j) = \frac{1}{N} p_a(S_j\to -S_j)
\end{equation}
as we choose the spin $S_j$ with probability $1/N$. Therefore we have
\begin{equation}
\frac{p(S_j\to -S_j)}{p(-S_j\to S_j)} = \frac{p_a(S_j\to -S_j)}{p_a(-S_j\to S_j)}.
\end{equation}
In the case where the change $S_j\to -S_j$ lowers the energy we have
\begin{equation}
p_a(S_j\to -S_j) =1,
\end{equation}
however the reverse move $-S_j\to S_j$ costs energy so 
\begin{equation}
p_a(-S_j\to S_j) =\exp\left[ -\beta\left(H(S_1, S_2,\cdots, S_j,\cdots S_N) - H(S_1, S_2,\cdots, -S_j,\cdots S_N)\right)\right],
\end{equation}
which gives
\begin{eqnarray}
\frac{p(S_j\to -S_j)}{p(-S_j\to S_j)} &=& \frac{1}{\exp\left[ -\beta\left(H(S_1, S_2,\cdots, S_j,\cdots S_N) - H(S_1, S_2,\cdots, -S_j,\cdots S_N)\right)\right]} \nonumber \\
&=&\frac{ \exp\left[ -\beta H(S_1, S_2,\cdots, -S_j,\cdots S_N)\right]}{\exp\left[ -\beta H(S_1, S_2,\cdots, S_j,\cdots S_N)\right]},
\end{eqnarray}
and so in this case we see that detailed balance is respected. In the case of a move which increases the energy it is easy to see that detailed balance is again respected.  

If we consider a case where the spins $+$ represent one type of particle and the $-$ another type and insist that the total  chemical composition remains the same the above dynamics is not correct as you cannot convert a $+$ into a $-$ and vice-a-versa. However a $+$ next to a $-$ can change places. Kawasaki dynamics chooses a neighbouring pair of $+$ and $-$ and tries to switch their positions, e.g. $.+-.\to .-+.$,  the move is accepted with probability $1$ if the energy change $\Delta E<0$  and with probability $p_a=\exp\left(-\beta\Delta E\right)$ if $\Delta E>0$.

Practically in a computer program if $\Delta E >0$ one draws a uniformly distributed random
number $r\in[0,1]$ (for example {\tt rand} in Fortran and Matlab), if $r< p_a $ the move accepted but if $r>p_a$ it is refused and the system stays in its initial state.

\section{Master equation for continuous time Markov chains}
In order to write a Markovian dynamics with continuous time we define
the transition probability from one state to another state as
\begin{equation}
\rho(C\to C') = W(C\to C')\Delta t,
\end{equation}
while the probability of staying in the same state is given by
\begin{equation}
\rho(C\to C) = 1-\sum_{C'\neq C}W(C\to C')\Delta t,
\end{equation}
in order to conserve probability. The variables $W(C\to C')$ are the rates of transition.
Detailed balance implies that we have to choose
\begin{equation}\boxed{
\frac{W(C'\to C)}{W(C \to C')} = \frac{\exp(-\beta E(C))}{\exp(-\beta E(C'))}.}
\end{equation} 
The evolution equation for $P(C,t)$ is then 
\begin{equation}
P(C,t+\Delta t)=  \left[ 1 - \sum_{C'\neq C}W(C\to C')\Delta t\right]P(C,t) + \sum_{C'\neq C} W(C'\to C) \Delta t P(C',t).
\end{equation}
Taking the limit $\Delta t\to 0$ we can write
\begin{equation}
P(C,t+\Delta t) = P(C,t) +\frac{dP(C,t)}{dt}\Delta t 
\end{equation}
and equating the coefficients of $\Delta t$ we find the {\em continuous time master equation}
\begin{equation}
\boxed{
\frac{dP(C,t)}{dt} =  \sum_{C'\neq C} W(C'\to C)  P(C',t) - W(C\to C')P(C,t).}
\end{equation}
This equation can be written as
\begin{equation}
\frac{dP(C,t)}{dt} =  \sum_{C} {\cal W}(C, C')  P(C',t) \label{wp}
\end{equation}
where the matrix $\cal{W}(C, C')$ is given by
\begin{equation}
{\cal W}(C, C') = W(C'\to C)(1-\delta_{CC'}) - \delta_{CC'}\sum_{C'\neq C} W(C\to C').
\end{equation}
The general solution to Eq. \eqref{wp} with initial condition $P(C,0)=P_0(C)$.
\begin{equation}
P(C,t) = \sum_{C'} \exp(t {\cal W})(C,C')P_0(C').
\end{equation}
As a special case we can consider the transition probability from an initial state $C_0$ to the final state in time $t$, denoted by $P(C|C_0,t)$. Here
we know $P(C,0) = \delta_{CC_0}$ and so
\begin{equation}
P(C|C_0,t) = \sum_{C'} \exp(t {\cal W})(C,C_0).\label{expw}
\end{equation}
Using this dynamics one can also ask what is the probability the system is in the state $C_1$ at time $t_1$ and state $C_2$ at time $t_2$ given that it started in the state $C_0$ at $t=0$ and $t_2>t_1>0$, which we denote by $P(C_2,t_2;C_1,t_1|C_0)$, by the Markov property it is simply given by
\begin{equation}
P(C_2,t_2;C_1,t_1|C_0)= P(C_2|C_1,t_2-t_1)P(C_1|C_0,t_1).
\end{equation}
This is because it must go from $C_0$ to $C_1$ in time $t_1$ then $C_1$ to $C_2$ in the time $t_2-t_1$.

A useful relationship is the Chapman-Kolmogorov formula which states for any $t<t'$
\begin{equation}
\boxed{
P(C_2|C_0,t) = \sum_{C_1} P(C_2|C_1,t-t_1)P(C_1|C_0,t_1),}
\end{equation}
this simply states that the trajectory going from $C_0$ to $C_2$ at time $t$ must have gone
through an intermediate state $C_1$ at time $t_1$. If we use the representation  Eq. \eqref{expw} we see that in terms of matrices ${\cal W}$ it means that
\begin{equation}
\exp\left[(t-t_1){\cal W}\right] \exp\left[t_1{\cal W}\right] = \exp\left[t{\cal W}\right],
\end{equation}
this can also be proved simply algebraically. 



Note also that if we start with an equilibrium probability distribution at $t=0$ then at time $t$
the system is still in equilibrium so
\begin{equation}
P(C,t) = \sum _{C'} P(C|C',t)P_{eq}(C) = P_{eq}(C),
\end{equation}
this is true algebraically because
\begin{equation}
[{\cal W} P](C) = \sum_{C'} {\cal W}(C,C') P_{eq}(C') = 0
\end{equation}
and so
\begin{equation}
[\exp(t{\cal W})P_{eq}](C) = P_{eq}(C).
\end{equation}
 If we consider the discrete time Markov process generated by the continuous time process
 viewed at the times $\tau =0, \ t, \ 2t, \ 3t \cdots \ nt$, the steady state distribution is still the equilibrium Gibbs-Boltzmann one but the probability of jumping from $C$ to $C'$ for this process in each discrete time step is $\rho(C'\to C') = P(C|C',t)$. However this discrete
 process must obey detailed balance and so we have
 \begin{equation}
 \frac{P(C|C',t)}{P(C'|C,t)} = \frac{P_{eq}(C)}{P_{eq}(C')} = \frac{\exp(-\beta E_C)}{\exp(-\beta E_{C'})}\label{db2}
 \end{equation}
 for any $t$. We also see that
 \begin{equation}
 P(C|C',t)P_{eq}(C') = P(C,t;C',0)
 \end{equation}
 i.e. the probability for an equilibrium system to be in state $C'$ at time 0 and $C$ at time $t$. The detailed balance property means that
 \begin{equation}
  P(C,t;C',0) = P(C',t;C,0).
 \end{equation}
 This means you are as probable to go from $C'$ to $C$ in time $t$ as from $C$ to $C'$ - this corresponds to the statistical invariance of dynamics on upon time reversal for an
 equilibrium system. 
\section{Correlations in time for Markov processes}
Consider some physical observable $A,\ B$ which takes the value $A(C),\ B(C)$ in the microstate 
$C$ (for example the energy $E(C)$). The correlation function between $A$ at time $t$ and
$B$ at an earlier time $t'$ is defined as
\begin{equation}
C_{AB}(t,t') = \langle A(t)B(t')\rangle .
\end{equation} 
In terms of the transition probabilities $P(C,t;C',t')$ to be in the microstate $C$ at time $t$ and microstate $C'$ at $t'$ we have
\begin{eqnarray}
\langle A(t)B(t')\rangle  = \sum_{C,C'} A(C)B(C') P(C,t;C',t)
\end{eqnarray} 
If the system starts in equilibrium we have
\begin{equation}
\langle A(t)B(t')\rangle  = \sum_{C,C',C_0} A(C)B(C') P(C|C';t-t')P(C'|C_0)P_{eq}(C_0)
\end{equation}
However $\sum_{C_0} P(C'|C_0)P_{eq}(C_0) = P_{eq}(C')$ and so
\begin{equation}
\langle A(t)B(t')\rangle= \sum_{C,C'} A(C)B(C') P(C|C';t-t')P_{eq}(C') = \langle A(t-t')B(0)\rangle,
\end{equation}
and we see the time-translational invariance of the correlation function in equilibrium. 
Using time translational invariance we can always set the time of the first observable 
to be $0$, and write simply
\begin{equation}
C_{AB}(t) = \langle A(t)B(0)\rangle = \sum_{C,C'} A(C)B(C') P(C|C';t)P_{eq}(C').\label{cab}
\end{equation}
If we use the detailed balance relation Eq. \eqref{db2} in Eq. \eqref{cab} we can write
\begin{equation}
C_{AB}(t) = \langle A(t)B(0)\rangle = \sum_{C,C'} A(C)B(C') P(C'|C;t)P_{eq}(C),
\end{equation}
now changing the labels $C\to C'$ and $C'\to C$ we obtain
\begin{equation}
C_{AB}(t) = \sum_{C,C',C_0} B(C) A(C')P(C|C';t)P_{eq}(C') = \langle B(t)A(0)\rangle =C_{BA}(t).
\end{equation}
An important consequence of this relation is that
\begin{equation}
C_{AB}(-t)= \langle B(0)A(-t)\rangle = \langle B(-t)A(0)\rangle = \langle B(0)A(t)\rangle = C_{AB}(t),
\end{equation}
where we have used the times translation invariance in equilibrium to write $\langle B(-t)A(0)\rangle = \langle B(-t+t)A(0+t )\rangle = \langle B(0)A(t)\rangle$. The correlation function is thus even in time.
\section{The fluctuation dissipation theorem}
Consider an  system in equilibrium whose microstates have energy $E(C)$ up till time 
$t=0$ at time $t=0$ we switch on a small perturbation so that the energy of each state becomes $E(C) -\epsilon A(C)$. Thus for $t>0$ we have that the probability distribution in the  presence of the perturbation $\epsilon A$ evolves as 
\begin{equation}
\frac{dP_{\epsilon}(C,t)}{dt} = \sum_{C'} {\cal W}_\epsilon(C,C') P_\epsilon(C',t)
\end{equation}
where ${\cal W}_\epsilon$ is given by the rates $W_\epsilon(C'\to C)$ which now obey the detailed balance with the new energy given by
\begin{equation}
\frac{W_\epsilon(C'\to C)}{W_\epsilon(C\to C')} = \frac{\exp\left(\beta \epsilon A(C)\right)P_{eq}(C)}{\exp\left(\beta \epsilon A(C')\right)P_{eq}(C')},\label{dba}
\end{equation}
where $P_{eq}(C)= \exp\left(-\beta E(C)\right)$ denotes the equilibrium distribution of the unperturbed system with $\epsilon=0$. We want to try and calculate how the perturbation applied
causes the average value another observable $B$ to change in time, this perturbation is written as
\begin{equation}
\langle \delta B(t)\rangle = \langle B(t)\rangle - \langle B(t)\rangle_{eq}.
\end{equation}
For a small perturbation $A$ we will find that
\begin{equation}
\langle \delta B(t)\rangle = \epsilon\int_0^t  dt' \chi_{BA}(t-t'),
\end{equation}
where $\chi_{BA}$ is called the response function of $B$ with respect to $A$.
In general $\epsilon$ can depend on time and in this case we have
\begin{equation}
\boxed{
\langle \delta B(t)\rangle = \int_0^t dt'\chi_{BA}(t-t')\epsilon(t),}\label{chiab}
\end{equation}
however in order to calculate $\chi_{BA}$ we only need to compute the response to a constant value $\epsilon$ but applied over an arbitrary period of time.

If we define $Q(C,t) = P_\epsilon(C,t) - P_{eq}(C)$ the we can write
\begin{eqnarray}
\langle \delta B(t)\rangle &=& \sum_{C} \left[ P_\epsilon(C,t) - P_{eq}(C)\right]B(C)\nonumber \\
&=& \sum_{C} Q(C,t)B(C)
\end{eqnarray}
The differential equation obeyed by $Q$  is
\begin{eqnarray}
&&\frac{dQ(C,t)}{dt} = \frac{dP_\epsilon(C,t)}{dt} = \sum_{C'} {\cal W}_\epsilon(C,C') P_\epsilon(C',t) \nonumber \\
&=& \sum_{C'} {\cal W}_\epsilon(C,C')[ Q(C',t) + P_{eq}(C')] \nonumber \\
&=& \sum_{C'} {\cal W}_\epsilon(C,C') Q(C',t) +\sum_{C'\neq C} [W_\epsilon(C'\to C)P_{eq}(C') - W_\epsilon(C\to C')P_{eq}(C)].\nonumber \\ \label{eqQ}
\end{eqnarray}
When $\epsilon$ is zero the last term above is zero by detailed balance, clearly when $A=0$ the 
solution is to Eq. \eqref{eqQ} $Q=0$ and so $Q$ if of $O(\epsilon)$. For small $\epsilon$ Eq. \eqref{dba} can be written as 
\begin{equation}
\frac{W_\epsilon(C'\to C)}{W_\epsilon(C \to C')} = \frac{[1+\beta \epsilon A(C)-\beta\epsilon A(C')] P_{eq}(C)}{P_{eq}(C')} +O(\epsilon^2)
\end{equation}
and this gives
\begin{eqnarray}
&&W_\epsilon(C'\to C)P_{eq}(C') - W_\epsilon(C\to C')P_{eq}(C)]\nonumber\\ &=& [1+\beta \epsilon A(C)-\beta\epsilon A(C')] W_\epsilon(C \to C')P_{eq}(C) -W_\epsilon(C\to C')P_{eq}(C)\nonumber \\
&=& [\beta \epsilon A(C)-\beta\epsilon A(C')] W_\epsilon(C \to C')P_{eq}(C).
\end{eqnarray}
Using this Eq. \eqref{eqQ} becomes
\begin{equation}
\frac{dQ(C,t)}{dt} = \sum_{C'} {\cal W}_\epsilon(C,C') Q(C',t) + \epsilon\beta\sum_{C'\neq C}[A(C)- A(C')] W_\epsilon(C \to C')P_{eq}(C) + O(\epsilon^2).
\end{equation}
Now we have seen that the solution of $Q$ above is $O(\epsilon)$ and so to $O(\epsilon)$ we can replace the terms ${\cal W}_\epsilon$ by the unperturbed ones ${\cal W}$ giving
\begin{equation}
\frac{dQ(C,t)}{dt} = \sum_{C'} {\cal W}(C,C') Q(C',t) + \beta\epsilon\sum_{C'\neq C}[ A(C)-A(C')] W(C \to C')P_{eq}(C) + O(\epsilon^2).\label{intq}
\end{equation}
It is useful to use detailed balance to write the last term on the right hand side as
\begin{eqnarray}
&&\sum_{C'\neq C}[ A(C)-A(C')] W(C \to C')P_{eq}(C) \nonumber \\
&=&
 \sum_{C'\neq C}W(C \to C')P_{eq}(C) A(C)- W(C' \to C)P_{eq}(C')A(C')\nonumber \\
 &=& -\sum_{C' C} {\cal W}(C,C') P_{eq}(C')A(C').
\end{eqnarray}
The Eq. \eqref{intq} is then becomes
\begin{equation}
\frac{dQ(C,t)}{dt} = \sum_{C'} {\cal W}_\epsilon(C,C') Q(C',t) - \epsilon\beta \sum_{C' C} {\cal W}(C,C') P_{eq}(C')A(C')+ O(\epsilon^2).
\end{equation}
However we have that $Q(C,t)=O(\epsilon)$ as $Q=0$ when $\epsilon=0$ and so we have
\begin{equation}
\frac{dQ(C,t)}{dt} = \sum_{C'} {\cal W}(C,C') Q(C',t) - \epsilon\beta \sum_{C' C} {\cal W}(C,C') P_{eq}(C')A(C')+ O(\epsilon^2).
\end{equation}

We then integrate this equation as follows
\begin{equation}
\frac{d}{dt}\left[\exp(-t{\cal W})(C,C') Q(C',t)\right] = - \epsilon\beta\sum_{C' C''}  \exp(-t{\cal W})(C,C''){\cal W}(C'',C') P_{eq}(C')A(C')
\end{equation}
and so
\begin{equation}
Q(C,t) = -\beta \epsilon\int_0^t \sum_{C',C''}\exp\left([t-t']{\cal W}\right)(C,C''){\cal W}(C'',C') P_{eq}(C')A(C'),
\end{equation}
where we obtain the integration constant by noting that $Q(C,t)=0$.
Now we notice that 
\begin{equation}
\sum_{C''}\exp\left([t-t']{\cal W}\right)(C,C''){\cal W}(C'',C')  = \frac{d}{dt}\exp\left([t-t']{\cal W}\right)(C,C'),
\end{equation}
or in matrix notation
\begin{equation}
\frac{d}{dt}\exp\left([t-t']{\cal W}\right)= \exp\left([t-t']{\cal W}\right)\cal{W}.
\end{equation}
This gives
\begin{equation}
Q(C,t) = -\beta \epsilon\int_0^t \sum_{C'}\frac{d}{dt}\exp\left([t-t']{\cal W}\right)(C,C') P_{eq}(C')A(C').
\end{equation}
Now we want to relate this to the correlation function 
\begin{equation}
C_{BA}(t) = \langle B(t) A(0)\rangle = \sum_{C,C'} P(C|C', t)P_{eq}(C') B(C)A(C')
= \sum_{C,C'} B(C) \exp\left(t{\cal W}\right)(C,C')  P_{eq}(C')A(C'),
\end{equation}
The perturbation to $B$ is then given by
\begin{equation}
\langle \delta B(t)\rangle = \sum_{C} Q(C,t)B(C) = -\beta \epsilon\int_0^t \sum_{C,C'} B(C)\frac{d}{dt}\exp\left(-[t-t']{\cal W}\right)(C,C') P_{eq}(C')A(C'),
\end{equation}
and so we see this can be written as
\begin{equation}
\langle \delta B(t)\rangle = -\beta\epsilon\int_0^t\frac{d}{dt} \langle B(t) A(t')\rangle dt',
\end{equation}
 now comparing with Eq. \eqref{chiab} gives, for $t>t'$
 \begin{equation}
 \chi_{BA}(t-t') =-\beta \frac{d}{dt}\langle B(t) A(t')\rangle =  -\beta \frac{d}{dt}C_{BA}(t-t')
 = \beta \frac{d}{dt'}C_{AB}(t-t')
 \end{equation}
For all times the {\em classical fluctuation dissipation} theorem states
\begin{equation}
\boxed{
\chi_{BA}(t)=  -\beta \theta(t)\frac{d}{dt}C_{BA}(t).}
\end{equation}
Taking the Fourier transform of the above gives
\begin{equation}
\tilde \chi_{BA}(\omega)=\int_{-\infty}^{\infty} dt \exp(i\omega t) \chi_{BA}(t) = \int_{-\infty}^{\infty} dt \cos(\omega t) \chi_{BA}(t) + i\int_{-\infty}^{\infty} dt \sin(\omega t) \chi_{BA}(t),
\end{equation}
thus
\begin{equation}
\tilde \chi''_{BA}(\omega) = \int_{-\infty}^{\infty} dt \sin(\omega t) \chi_{BA}(t) 
\end{equation}
and using the fluctuation dissipation theorem we find
\begin{equation}
\tilde \chi''_{BA}(\omega) = -\beta \int_{-\infty}^{\infty} dt \sin(\omega t)  \theta(t)\frac{d}{dt}C_{BA}(t)
\end{equation}
Integrating by parts now gives
\begin{equation}
\tilde \chi''(\omega) = \beta\omega \int_0^\infty dt \cos(\omega t) C_{BA}(t)= \frac{\beta\omega}{2} \int_0^\infty dt \left[ \exp(i\omega t)+\exp(-i\omega t)\right]  C_{BA}(t)
\end{equation}
Now we use $C_{BA}(t)= C_{BA}(-t)$ to find
\begin{equation}
\boxed{
\tilde \chi_{BA}''(\omega) = \frac{\beta\omega}{2}\tilde C_{BA}(\omega)}.\label{fdtft}
\end{equation}
Due to causality the response function $\chi_{AB}(t)=0$ for $t<t'$. We can write
\begin{equation}
\chi_{AB}(t) =\frac{1}{2\pi}\int_{-\infty}^\infty d\omega \exp(-i\omega t) \tilde \chi(\omega),
\end{equation}
when $t<0$ the integral long the real axis can be carried out along the contour $\omega=R\exp(i\theta)$ as $R\to \infty$ and with $\theta\in[0,\pi]$ i.e above the real axis. The integral gives zero as long as there are no poles in the upper half of the complex plane.

\section{Applications of the fluctuation dissipation theorem}
In Eq. \eqref{fdt1} found a relationship between the correlation function $C(t) = \langle X(t) X(0)\rangle$ and the response to force $f$ for a particle in a Harmonic trap. In the language used above this means that $B(t)=X(t)$. Applying a force to a particle sends the energy
from $E(X)$ to $E(X)-fX$ where $f$ corresponds to the small parameter $\epsilon$. This means that $A=X$. The correlation function in the fluctuation dissipation relation is therefore
\begin{equation}
C_{BA}(t) = C_{XX}(t) = \langle X(t) X(0)\rangle,
\end{equation}
and using Eq. \eqref{fdtft} recovers Eq. \eqref{fdt1}.

As another example consider a one dimensional electrical conductor with charges $q$ 
at positions $X_i$. We consider the effect of applying a small, time dependent, electric field ${\cal E}(t)$. Doing this changes the energy of a state by $E(\{X_i\}) \to  E(\{X_i\}) - {\cal E}(t)q\sum_i X_i$. The
average electric current per unit volume is given by
\begin{equation}
j = \frac{q}{\cal V}\sum_i \frac{dX_i}{dt},
\end{equation}
where ${\cal V}$ is the volume of the system. In the notation used here $B = j$ and $A =q\sum_i X_i$ while $ {\cal E}$ plays the role of the small parameter $\epsilon$. The fluctuation dissipation theorem can be rewritten as
\begin{equation}
 \chi_{AB}(t) = -\beta\theta(t) \langle \frac{dB(t)}{dt} A(0)\rangle = \beta\theta(t) \langle B(t)\frac{dA(0)}{dt} \rangle\label{fdta}
\end{equation}
The average current is given by
\begin{equation}
j(t) = \int_{-\infty}^t  \chi_{jA}(t-t'){\cal E}(t'),
\end{equation}
and Fourier space it reads
\begin{equation}
{\tilde j}(\omega) = \tilde\chi_{jA}(\omega)\tilde{\cal E}(\omega).
\end{equation}
The frequency dependent version of Ohm's law takes the form
\begin{equation}
\tilde j(\omega) = \sigma(\omega) \tilde{\cal E}(\omega)
\end{equation}
so we can identify $\sigma(\omega)=\tilde\chi_{jA}(\omega)$ as the frequency dependent conductivity.
From the version  of fluctuation dissipation theorem in Eq. \eqref{fdta} we find
\begin{equation}
\tilde\chi_{BA}(\omega) = \int_0^\infty dt \exp(i\omega t) \beta \langle B(t)\frac{dA(0)}{dt} \rangle.
\end{equation} 
Recall that $B(t)=j(t)$, while
\begin{equation}
\frac{dA(0)}{dt} = q\sum_i \frac{dX(0)}{dt} = {\cal V}j(0).
\end{equation}
This gives
\begin{equation}
\boxed{
\sigma(\omega) = \beta {\cal V}\int_0^\infty dt \exp(i\omega t) \langle j(t)j(0) \rangle,}\label{nyq}
\end{equation} 
 the Eq. (\ref{nyq}) is known as {\em Nyquist's theorem}, it is an example of a {\em Green-Kubo formula}, which gives a transport coefficient (in this case a conductivity) as an integral
 of a correlation function. 

The problem is now to calculate
\begin{equation}
\langle j(t)j(0)\rangle = \frac{q^2}{{\cal V}^2} \sum_{ij} \langle \frac{dX_i(t)}{dt}\frac{dX_j(0)}{dt}\rangle .
\end{equation}
If we assume that the velocities of the electrons are not correlated this gives
\begin{equation}
\langle \frac{dX_i(t)}{dt}\frac{dX_j(0)}{dt}\rangle =0
\end{equation}
for $i\neq j$. We can also write using the equipartition of energy
\begin{equation}
\langle \frac{dX_i(t)}{dt}\frac{dX_i(0)}{dt}\rangle =\frac{k_B T}{m} c(t),
\end{equation}
where $c(t)$ is a decaying function with $c(0)=0$. This then gives
\begin{equation}
\langle j(t)j(0)\rangle = \frac{nq^2}{{\cal V}} \frac{k_B T}{m} c(t) ,
\end{equation}
where $n = N/{\cal V}$ is the number density of electrons per unit volume ($N$ being the total number) so
\begin{equation}
\sigma(\omega) = \frac{nq^2}{m}\int_0^\infty dt \exp(i\omega t) c(t)
\end{equation}
If we assume a simple exponential decay, known as {\em Debye relaxation},
\begin{equation}
c(t) =\exp(-\frac{t}{\tau_c}),
\end{equation}
where $\tau_c$ is a relaxation time, we obtain
\begin{equation}
\sigma(\omega) = \frac{nq^2}{m}\frac{\tau_c}{1-i\omega\tau_c}.
\end{equation}

As another example we consider the dynamics of a dipole in the presence of a time dependent electric field. The polarisability tensor  of the dipole is given by the formula
\begin{equation}
\langle p_i(t)\rangle =\int_{\infty}^t \chi_{ij}(t-t') E_i(t') dt',
\end{equation}
where $E_i(t)$ is the $i$-th component of the electric field at time $t$. In Fourier space we have
\begin{equation}
\langle \tilde p_i(\omega)\rangle  = \tilde \chi_{ij}(\omega)\tilde E_j(\omega).
\end{equation}
The presence of the dipole shifts the energy of the system to  $\Delta{\cal E} = -{\bf p}\cdot{\bf E}$. If we consider the case where  ${\bf E}$ is only nonzero in the direction $j$ and so $E_j=\epsilon $, $A=p_j $ and we choose $B= p_i$ the fluctuation dissipation theorem gives
\begin{equation}
\tilde \chi''_{BA}(\omega) = \tilde \chi''_{ij}(\omega)=\frac{\beta \omega}{2}\tilde C_{BA}(\omega),
\end{equation}
where 
\begin{equation}
C_{BA}(t-t')= \langle p_i(t)p_j(t')\rangle = C_{ij}(t-t')
\end{equation}
where $C_{ij}(t-t')$ 
is the correlation function between the components of the electric dipole. We thus find
\begin{equation}
\tilde \chi''_{ij}(\omega)=\frac{\beta \omega}{2}\tilde C_{ij}(\omega),
\end{equation}
this means that the fluctuations of the dipole are related to its polarisability.
\chapter{Various Applications}

\section{The Glauber solution to the one dimensional Ising model}
The Ising model in one dimension has Hamiltonian
\begin{equation}
H = -J\sum_{i} \sigma_i\sigma_{i+1},
\end{equation}
where the spins $\sigma_i=\pm1$.
The equilibrium statistical mechanics can be solved by using transfer matrix but the dynamics cannot in general be solved. However a certain form of dynamics called Glauber dynamics can be solved exactly. The dynamics we consider is nonconserved dynamics, we change the  spin $\sigma_i$ at site $i$ to $-\sigma_i$ with rate
\begin{equation}
W(\sigma_i\to-\sigma_i) = w_0\frac{\exp(-\beta H(\sigma_1,\ \sigma_2,\ \cdots -\sigma_i,\cdots, \sigma_N)}{\exp(-\beta H(\sigma_1,\ \sigma_2,\ \cdots,\  -\sigma_i,\cdots, \sigma_N)+ \exp(-\beta H(\sigma_1,\ \sigma_2,\ \cdots ,\ \sigma_i,\cdots, \sigma_N)}.
\end{equation}
This choice of rates is known as Glauber dynamics and respects detailed balance as clearly
\begin{equation}
\frac{W(\sigma_i\to-\sigma_i)}{W(\sigma_i\to-\sigma_i)} = \frac{\exp(-\beta H(\sigma_1,\ \sigma_2,\ \cdots, -\sigma_i,\cdots,\ \sigma_N)}{\exp(-\beta H(\sigma_1,\ \sigma_2,\ \cdots,\ \sigma_i,\cdots, \sigma_N)}.
\end{equation}
The energy change on flipping spin $\sigma_i$ to $-\sigma_i$ is given by
\begin{equation}
\Delta H[\sigma_i\to -\sigma_i]  = -J[-\sigma_{i-1}\sigma_i + -\sigma_{i+1}\sigma_i] + J[\sigma_{i-1}\sigma_i + \sigma_{i+1}\sigma_i]
= 2J[\sigma_{i-1} + \sigma_{i+1}]\sigma_i.
\end{equation}
Using this we find that
\begin{equation}
W(\sigma_i\to-\sigma_i)= w_0\frac{\exp(-\beta \Delta H[\sigma_i\to -\sigma_i])}
{1+ \exp(-\beta \Delta H[\sigma_i\to -\sigma_i])}= \frac{\exp(-2\beta h_i\sigma_i)}
{1+ \exp(-2\beta h_i\sigma_i)}=\frac{\exp(-\beta h_i\sigma_i)}
{\exp(\beta h_i\sigma_i)+ \exp(-\beta h_i\sigma_i)} ,
\end{equation}
where $h_i = J[\sigma_{i-1} + \sigma_{i+1}]$ is the local field acting on the spin $\sigma_i$. Now we write
\begin{equation}
\exp(-\beta h_i\sigma_i)=\frac{1+\sigma_i}{2}\exp(-\beta h_i)+\frac{1-\sigma_i}{2}\exp(\beta h_i) = \cosh(\beta h_i)-\sigma_i \sinh(\beta h_i),
\end{equation}
and so
\begin{equation}
W(\sigma_i\to-\sigma_i)= \frac{w_0}{2}(1- \sigma_i \tanh(\beta h_i)).
\end{equation}
We now consider the term
\begin{equation}
\tanh(\beta h_i) = \tanh(\beta J [\sigma_{i-1}+ \sigma_{i+1}])
= \frac{1}{2}[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J),
\end{equation}
using the fact that the spin values are $\pm 1$. This then gives
\begin{equation}
W(\sigma_i\to-\sigma_i)= \frac{w_0}{2}(1-  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J)).
\end{equation}

The master equation for the system is given by
\begin{equation}
\frac{dP(\{\sigma_i\},t)}{dt} = \sum_i W(-\sigma_i\to \sigma_i)P(\sigma_1,\ \sigma_2,\cdots, -\sigma_i\cdots \sigma_N)- \sum_i W(\sigma_i\to -\sigma_i)P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N),
\end{equation}
and using the explicit rates above we find
\begin{eqnarray}
\frac{dP(\{\sigma_i\},t)}{dt} &=& \sum_i \frac{w_0}{2}(1+  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, -\sigma_i\cdots \sigma_N)- \\
&&\sum_i \frac{w_0}{2}(1-  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N).
\end{eqnarray}
The average value of the spin $\sigma_j$, $\langle \sigma_j\rangle $,  is given by
\begin{equation}
\langle \sigma_j\rangle = \sum_{\sigma_i=\pm1}\sigma_j P(\{\sigma_j\},t),
\end{equation}
if we multiply the master equation by $\sigma_j$ and then sum over all the possible values of $\sigma_i$ we find
\begin{eqnarray}
\frac{d\langle \sigma_j\rangle }{dt}&=& \sum_{\sigma_i=\pm 1}\sigma_j\frac{dP(\{\sigma_i\},t)}{dt} = \sum_{\sigma_i=\pm1}\sum_i \sigma_j\frac{w_0}{2}(1+  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, -\sigma_i\cdots \sigma_N) \\
&&-\sum_{\sigma_i=\pm1}\sum_i \sigma_j\frac{w_0}{2}(1-  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N).
\end{eqnarray}
Now we notice that we can write this as 
\begin{eqnarray}
&&\frac{d\langle \sigma_j\rangle }{dt}=\\ &&\sum_{\sigma_i=\pm 1}\sigma_j\frac{dP(\{\sigma_i\},t)}{dt} = \sum_{\sigma_i=\pm1}\sum_i (-1)^{\delta_{ij}}\sigma_j\frac{w_0}{2}(1-  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N) \\
&&-\sum_{\sigma_i=\pm1}\sum_i \sigma_j\frac{w_0}{2}(1-  \frac{1}{2}\sigma_i[\sigma_{i+1}+\sigma_{i-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N).
\end{eqnarray}
Written this way we see that in the sums over $i$ above all terms cancel apart from the terms where $i=j$, this gives
\begin{equation}
\frac{d\langle \sigma_j\rangle }{dt} = -w_0\sum_{\sigma_i=\pm1}\sigma_j(1-  \frac{1}{2}\sigma_j[\sigma_{j+1}+\sigma_{j-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N),
\end{equation}
this then gives
\begin{equation}
\frac{d\langle \sigma_j\rangle }{dt} = -w_0\langle \sigma_j\rangle+ \frac{w_0}{2}[\langle \sigma_{j+1}\rangle +\langle\sigma_{j-1}\rangle ]\tanh(2\beta J))
\end{equation}
Thus if the system has an initial uniform magnetisation $m=\langle \sigma_1\rangle$, the system is invariant by translation and so $\langle \sigma_i(t)\rangle =m(t)$ and we find
\begin{equation}
\frac{dm(t) }{dt} = -w_0m(t)[1-\tanh(2\beta J)],
\end{equation}
thus if the initial magnetisation is $m_0$ we have
\begin{equation}
m(t) = m_0 \exp(-w_0[1-\tanh(2\beta J)]t).
\end{equation}
We thus see that when the temperature is low, the magnetisation decays
slowly to zero with a characteristic time scale
\begin{equation}
\tau = \frac{1}{w_0[1-\tanh(2\beta J)]}.
\end{equation}
We now use the same method to compute the correlation function between two spins at sites $j$ and $k$, $\langle \sigma_j(t)\sigma_k(t)\rangle$,
\begin{eqnarray}
&&\frac{d\langle \sigma_j\sigma_k\rangle }{dt}=\sum_{\sigma_i=\pm 1}\sigma_j\sigma_k\frac{dP(\{\sigma_i\},t)}{dt}=\\ &&  \\
&&-2\sum_{\sigma_i=\pm1} (-1)^{\delta_{ij}}(-1)^{\delta_{ik}}\sigma_j\sigma_k\frac{w_0}{2}(1-  \frac{1}{2}\sigma_j[\sigma_{j+1}+\sigma_{j-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N)\\
&&-2\sum_{\sigma_i=\pm1} \sigma_j\sigma_k\frac{w_0}{2}(1-  \frac{1}{2}\sigma_k[\sigma_{k+1}+\sigma_{k-1}]\tanh(2\beta J))P(\sigma_1,\ \sigma_2,\cdots, \sigma_i\cdots \sigma_N).
\end{eqnarray}
In the above only the terms $i=j$ and $i=k$ give non-zero contributions and 
we find
\begin{equation}
\frac{d\langle \sigma_j\sigma_k\rangle }{dt}= -2w_0 \langle \sigma_j\sigma_k\rangle + \frac{w_0\tanh(2\beta J)}{2}(\langle \sigma_k\sigma_{j+1}\rangle +\langle\sigma_k\sigma_{j-1}\rangle+\langle \sigma_j\sigma_{k+1}\rangle +\langle \sigma_j\sigma_{k-1}\rangle).
\end{equation}
If we assume that the system is invariant under translation then we can write
\begin{equation} 
\langle \sigma_j\sigma_k\rangle = c(j-k),
\end{equation}
and we obtain
\begin{equation}
\frac{d\langle c(j-k)\rangle }{dt}= -2w_0 \langle c(j-k)\rangle + \frac{w_0\tanh(2\beta J)}{2}(c(j+1-k) +c(j-1-k)+ c(j-k-1) +c(j-k+1)).
\end{equation}
writing $j-k=r$ we find 
\begin{equation}
\frac{d c(r) }{dt}= -2w_0  c(r) + w_0\tanh(2\beta J)(c(r+1) +c(r-1)).
\end{equation}
Note that we have the boundary condition $c(0)=\langle \sigma^2_1\rangle =1$.
The equilibrium correlation function is given by
\begin{equation}
-2  c_0(r) + \tanh(2\beta J)(c_0(r+1) +c_0(r-1)),
\end{equation}
and if we look for a solution $c_0(r) =z^r$ we find
\begin{equation}
z^2 \tanh(2\beta J)-2 z  +\tanh(2\beta J) =0
\end{equation}
which has solutions
\begin{equation}
z= \frac{1 \pm \sqrt{1-\tanh^2(2\beta J)}}{\tanh(2\beta J)}=\frac{1\pm \frac{1}{\cosh(2\beta J)}}{\tanh(2\beta J)}.
\end{equation}
Now as we expect the correlation function to decay for large $r$ we must have $z<1$ and so we chose the sign $-$, which gives
\begin{equation}
z = \frac{\cosh(2\beta J)-1}{\sinh(2\beta J)}= \tanh(\beta J).
\end{equation}
If $z = \exp(-r/\xi)$ where $\xi$ is the correlation length we find
\begin{equation}
\xi = -\frac{1}{\ln(\tanh(\beta J)}.
\end{equation}
This agrees with the equilibrium calculation. The correlation function always converged to this result at finite temperatures, however when $T=0$ we see that the correlation length diverges. This means that the correlation length must grow with time. Setting $\beta=\infty$ we find
\begin{equation}
\frac{d c(r) }{dt}= -2w_0  c(r) + w_0(c(r+1) +c(r-1)).
\end{equation}
Now if $a$ is the distance between the spins we can write $x=ra$ and the above equation becomes
\begin{equation}
\frac{d c(r) }{dt}= w_0a^2\frac{ -2 c(r) + w_0(c(r+1) +c(r-1))}{a^2},
\end{equation}
which becomes a continuum equation as $a\to 0$
\begin{equation}
\frac{\partial c(x) }{\partial t}= D \frac{\partial^2}{\partial x^2}c(x).
\end{equation}
This is just the diffusion equation with $D= w_0a^2$. We look for a {\em scaling} solution of the form $c(x)= f(x/t^z)$ for large $t$ so the correlation length grows as $t^z$. The exponent $z$ is called the {\em growth exponent} (actually normally $z_d=1/z$ is called the growth exponent in the literature). Substituting this into the diffusion equation we find
\begin{equation}
-z\frac{x}{ t^{z+1}}c'(\frac{x}{t^z}) = \frac{D}{t^{2z}} c''(\frac{x}{t^z}),
\end{equation}
in terms of the scaling variable $y=x/t^z$ this becomes
\begin{equation}
-\frac{zy c'(y)}{t} = \frac{D}{t^{2z}} c''(y),
\end{equation}
and so the solution works if $2z=1$ and so $z=1/2$. We then have the scaling function $c(y)$ which obeys
\begin{equation}
c''(y) +\frac{1}{2D} yc'(y) = 0,
\end{equation}
and the solution must be chosen so that $c(0)=1$ and $c(y)\to 0$ as $y\to\infty$. Integrating once then gives
\begin{equation}
c'(y) = C\exp(-\frac{1}{4D}y^2),
\end{equation}
where $C$ is a constant. Integrating a second time then gives
\begin{equation}
c(y) = B+ C\int_{0}^y du \exp(-\frac{1}{4D}u^2).
\end{equation}
The boundary condition $c(0)=1$  then gives $B=1$, and the condition $c(y)\to 0$ as $y\to\infty$ gives
\begin{equation}
 C=-\frac{1}{\int_{0}^\infty du \exp(-\frac{1}{4D}u^2)},
\end{equation}
so
\begin{equation}
c(y) = 1- \frac{\int_{0}^y du \exp(-\frac{1}{4D}u^2)}{\int_{0}^\infty du \exp(-\frac{1}{4D}u^2)}.
\end{equation}


\section{Random Matrix Theory and Dyson Brownian motion}
The theory of random matrices was initially invented in statistics, however it was Wigner who introduced the theory in physics. Wigner postulated that if a system is sufficiently complex its Hamiltonian becomes equivalent to a random Hamiltonian that satisfies the symmetries of the original non random problem. Much later Bohigas, Gionnani and Schmidt conjectured that a system which is classically chaotic has a quantum Hamiltonian with random matrix theory statistics. Random matrix like level spacing statistics are found in a surprising number of situations - see Fig. (\ref{figls}).

For a quantum system with $N$ states we can represent the Hamiltonian by an $N\times N$ matrix $M$
\begin{equation}
H = M,
\end{equation}
for a real Hamiltonian we must have that 
$M=M^T$ to ensure that $H$ is self adjoint and thus has real eigenvalues.
We also assume that the Hamiltonian has certain symmetries, for instance
\begin{equation}
H'= O^THO \equiv H
\end{equation}
where $\equiv$ indicates statistically equivalent and $O$ is an arbitrary rotation in the space of states (so $O$ is orthogonal and $O^TO=I$). The simplest matrix ensemble in this case is
the Gaussian Orthogonal Ensemble or GOE. The elements of the matrix $M$ are taken to be distributed according to
\begin{equation}
P(M)= C_N \exp(-\frac{1}{2}{\rm Tr M^2}) = C_N\exp(-\frac{1}{2}\sum_{i=1}^N M_{ii}^2 - \sum_{i<j}M_{ij}^2).
\end{equation}
Note that if $M'= O^T M O$, where $O$ is an orthogonal matrix, then  ${\rm Tr} M'^2 =  {\rm Tr} O^TMOO^TMO ={\rm Tr} M^2$. We now recall that Gaussian random variables have probability density function
\begin{equation}
p(x) =\frac{1}{\sqrt{2\pi \sigma^2}}\exp(-\frac{1}{2\sigma^2}x^2)
\end{equation}
where $\sigma^2 =\langle X^2\rangle$ is the variance. For the matrix $M$ the elements $M_{ij}$ have variance
\begin{equation}
\sigma_{ii}^2 = 1, \ \sigma_{ij}^2 =\frac{1}{\sqrt{2}} \ {\rm for}\ i\neq j.
\end{equation}
The normalisation factor $C_N$ is therefore given by
\begin{equation}
C_N = \frac{1}{(2\pi)^{\frac{N}{2}}}\frac{1}{\pi^{\frac{N(N-1)}{4}}} = \frac{1}{2^{\frac{N}{2}}\pi^{\frac{N(N+1)}{4}}}.
\end{equation}
Now if we consider the process,
\begin{equation}
dX_t = -X_t dt +\sqrt{2\sigma} dB_t
\end{equation}
we see that the probability density function satisfies
\begin{equation}
\frac{\partial  p(x,t)}{\partial t} =\frac{\partial}{\partial x}[\sigma^2 \frac{\partial  p(x,t)}{\partial x} + x p(x,t)],
\end{equation}
and the equilibrium distribution is given by
\begin{equation}
p_{eq}(x) = \frac{\exp(-\frac{x^2}{2\sigma^2})}{\sqrt{2\pi\sigma^2}}.
\end{equation}
Therefore if we consider the matrix stochastic differential equation
\begin{eqnarray}
dM_{ii} &=& -M_{ii} dt+ \sqrt{2} dB_{ii} \\
dM_{ij} &=& -M_{ij} dt +  dB_{ij}
\end{eqnarray}
the statistics of the matrix $M$ in equilibrium is exactly the $GOE$ statistics.
We rewrite the stochastic differential equation as
\begin{equation}
dM_{ij} = -M_{ij}dt + dW_{ij}
\end{equation}
where $\langle dW_{ij}\rangle=0$ and 
\begin{equation}
\langle dW_{ij}dW_{kl}\rangle = dt(\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk}).
\end{equation}
Using quantum mechanics notation, for the eigenvector $\ket{n}$ corresponding to eigenvalue $\lambda_n$ we have 
\begin{equation}
M\ket{n} =\lambda_n\ket{n}
\end{equation}
Clearly $\ket{n}$ and $\lambda_n$ depend on $t$ as $M$ depends on $t$.
Second order perturbation theory from quantum mechanics gives the evolution of a given $\lambda_n$ as 
\begin{equation}
d\lambda_n = \bra{n} dM\ket{n} + \sum_{m\neq n} \frac{\bra{n} dM\ket{m}\bra{m} dM\ket{n}}{\lambda_n-\lambda_m} +O(dt^{\frac{3}{2}}),
\end{equation}
note, as usual with Ito calculus,  we need to take the second order term as it is of order $dt$. Simplifying we find
\begin{equation}
d\lambda_n = -\bra{n} M\ket{n} dt +\bra{n} dW\ket{n} + \sum_{m\neq n} \frac{\bra{n} dW\ket{m}\bra{m} dW\ket{n}}{\lambda_n-\lambda_m} .
\end{equation}
The first term on the RHS above is easy to evaluate as we have $\bra{n}M\ket{n}= \lambda_n$. We also see that
\begin{equation}
\langle \bra{n}dW\ket{n}\rangle=0.
\end{equation}
We now write the $N$ components of the vectors ${\bf v}_n=\ket{n}$ as $v_{ni}$ where $1\leq i\leq N$. In this notation, and first using the Ito rule, we find 
\begin{eqnarray}
\bra{n} dW\ket{m}\bra{m} dW\ket{n}&=&\langle \bra{n} dW\ket{m}\bra{m} dW\ket{n}\rangle \\
&=& \langle \sum_{ijkl} v_{ni}dW_{ij} v_{mj}v_{mk} dW_{kl}v_{nl}\rangle \\
&=&\sum_{ijkl} v_{ni} v_{mj}v_{mk} v_{nl}\langle dW_{ij}dW_{kl}\rangle\\
&=& \sum_{ijkl} v_{ni} v_{mj}v_{mk} v_{nl}[\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk}] \\
&=& \sum_{ij}v_{ni} v_{mj}v_{mi} v_{nj} + v_{ni} v_{mj}v_{mj} v_{ni}\\
&=& [{\bf v}_n\cdot {\bf v}_m]^2 + [{\bf v}_n \cdot {\bf v}_n][{\bf v}_m \cdot {\bf v}_m]\\
&=& \delta_{mn} + 1
\end{eqnarray}

The noise term $dU_n = \bra{n} dW\ket{n}$ clearly has $\langle dU_n\rangle =0$ and also we have
\begin{eqnarray}
dU_ndU_m &=& \langle dU_n dU_m\rangle \\
&=& \langle \bra{n} dW\ket{n}\bra{n} dW\ket{n}\rangle \\
&=& \langle \sum_{ijkl} v_{ni}dW_{ij} v_{nj}v_{mk} dW_{kl}v_{ml}\rangle\\
&=&  \sum_{ijkl} v_{ni}v_{nj}v_{mk} v_{ml}[\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk}]\\
&=&\langle \sum_{ij}v_{ni}v_{nj}[v_{mi} v_{mj}+v_{mj} v_{mi}]\\
&=& 2 [{\bf v}_n \cdot{\bf v}_n]^2 = 2 \delta_{mn}
\end{eqnarray}
The noise terms for different eigenvalues are thus independent and we can write $dU_n =\sqrt{2}dB_n$ where $dB_n dB_m = \delta_{nm}dt$. 

Putting all these results together we find the equation of so called {\em Dyson Brownian motion}
\begin{equation}
d\lambda_n = -\lambda_n dt + dt \sum_{m\neq n} \frac{1}{\lambda_n-\lambda_m}
+\sqrt{2}dB_n.
\end{equation}
This can then be written as
\begin{equation}
d\lambda_n = -\frac{\partial V(\boldsymbol{\lambda})}{\partial \lambda_n}
+\sqrt{2}dB_n,
\end{equation}
where the potential $V$ is given by
\begin{equation}
V(\boldsymbol{\lambda})=\frac{1}{2}\sum_{n}\lambda_n^2 - \sum_{n<m}\ln(|\lambda_n-\lambda_m|).
\end{equation}
This means that the eigenvalues of a GOE matrix have the same distribution as Brownian particles confined in a quadratic potential and interacting via the pairwise potential $v(x)= -\ln(|x|)$ when the are separated by a distance $x$. This potential is repulsive and prevents the eigenvalues from crossing each other. The whole system is prevented from exploding by the confining harmonic potential.
Note that here we have $k_BT=1$ and so the equilibrium probability distribution is given by
 \begin{equation}
 P_{eq}(\boldsymbol{\lambda})=\frac{1}{Z_N}\exp(-V(\boldsymbol{\lambda})),
 \end{equation}
 where 
 \begin{equation}
 Z_N= \int d\boldsymbol{\lambda}\exp(-V(\boldsymbol{\lambda})),
 \end{equation}
 is the canonical partition function.
 From this we can write
 \begin{equation}
 P_{eq}(\boldsymbol{\lambda})=\frac{1}{Z_N}[\prod_{n<m}|\lambda_m-\lambda_n|]\exp(-\frac{1}{2}\sum_n \lambda_n^2).
 \end{equation}
 We thus see that $P_{eq}(\boldsymbol{\lambda})=0$ if $\lambda_n=\lambda_m$ for any pair of different eigenvalues.
 
 Consider the case $N=2$, here we find that
 \begin{equation}
 P_{eq}(\lambda_1, \lambda_2) = \frac{1}{Z_2}|\lambda_1-\lambda_2|\exp(
 -\frac{\lambda_1^2 +\lambda_2^2}{2}).
 \end{equation}
 This means that the probability distribution function for the distance $\Delta=|\lambda_1-\lambda_2|$ and $S= \lambda_1+\lambda_2$ is given by
 \begin{equation}
 P_{eq}(\Delta,S ) = \frac{1}{Z'_2}\Delta\exp(
 -\frac{\Delta^2 +S^2}{4}),
 \end{equation}
 note that the Jacobian of the transformation to these coordinates is constant. From this we find that
 \begin{equation}
 P_{eq}(\Delta) = \frac{\Delta}{2} \exp(-\frac{\Delta^2}{4}),
\end{equation}
and the probability distribution is properly normalised. For large $N$ the distribution of distances between the ordered  eigenvalues $\lambda_1,\ \lambda_2 \cdots \lambda_{n-1}, \ \lambda _n \cdots \lambda_N$, if we define 
\begin{equation}
S =\frac{ \lambda_n - \lambda_{n-1}}{\langle  \lambda_n - \lambda_{n-1}\rangle}
\end{equation}
 then the probability  distribution of $S$ approximately given by the {\em Wigner surmise} 
\begin{equation}
p(s) = \frac{\pi s}{2}\exp(-\frac{\pi}{4}s^2).
\end{equation}
We now come to the question of what the overall density of eigenvalues looks like. We define the density as
\begin{equation}
\rho(\lambda) = \sum_{i=1}^N \delta(\lambda-\lambda_n).
\end{equation}
We now use a remarkably useful representation of the delta function, this is used in many contexts especially in quantum mechanics where it is used to compute the density of states from the Green's function of the Schr\"odinger equation,
\begin{equation}
\delta(x) = \frac{1}{\pi}\lim_{\epsilon \to 0^+} \frac{\epsilon}{x^2 + \epsilon^2}.
\end{equation} 
To see this notice that as $\epsilon\to 0^+$ that $f(x) = \frac{\epsilon}{x^2 + \epsilon^2}$ goes to zero for $x>0$ but diverges as $x\to 0$, it is also positive everywhere. NoW consider
\begin{equation}
I_g = \frac{1}{\pi}\int_{-\infty}^{\infty} dx g(x) \frac{\epsilon}{x^2 + \epsilon^2} ,
\end{equation}
making the change of variables $y= x/\epsilon$ we find
\begin{equation}
I_g =\frac{1}{\pi} \int_{-\infty}^{\infty} dy g(\epsilon y) \frac{1}{y^2 + 1} 
\end{equation}
and so as $\epsilon\to 0$ we find
\begin{equation}
I_g = g(0) \frac{1}{\pi}\int_{-\infty}^{\infty} dy  \frac{1}{y^2 + 1} = g(0),
\end{equation}
which is exactly the property required for a delta function. Notice that we can also write
\begin{equation}
\frac{\epsilon}{x^2 + \epsilon^2} ={\rm Im} \frac{1}{x-i\epsilon},
\end{equation}
and so
\begin{equation}
\delta(x) = \frac{1}{\pi}\lim_{\epsilon \to 0^+} {\rm Im}\frac{1}{x-i\epsilon}.
\end{equation} 
Therefore to compute the eigenvalue density, we consider the resolvent
\begin{equation}
R(z) = (z-M)^{-1} = R^T(z),
\end{equation}
which inherits the symmetry of the matrix $M$. Of particular interest is the trace of the resolvent
\begin{equation}
r(z) = {\rm Tr} \ R(z) = \sum_{i} \frac{1}{z-\lambda_i}
\end{equation}
where the $\lambda_i$ denotes the $N$ real eigenvalues for matrices which are $N\times N$.
The density of eigenvalues is given by
\begin{equation}
 \sum_i \delta(z-\lambda_i) =\frac{1}{\pi}{\rm Im} \ r(z)   .
\end{equation}
where $z$ has an infinitesimally negative imaginary part, {\em i.e} $z=\lambda-i\epsilon$.

The stochastic equation obeyed by $R$ is given, using the Ito Calculus, by
\begin{eqnarray}
dR = (z-M-dM)^{-1} - (z-M)^{-1} &=& (z-M)^{-1}\left[ (I - dM (z-M)^{-1})^{-1} -I\right]\nonumber \\
&=& (z-M)^{-1}\left[ dM (z-M)^{-1} + dM (z-M)^{-1}dM (z-M)^{-1}\right]
\end{eqnarray}
and using the stochastic differential equation  for $M$ we find
\begin{equation}
dR = R [-M dt + dW] R + \langle R dW R dW R\rangle.
\end{equation}
The second Ito term can be evaluated as 
\begin{eqnarray}
\langle R_{ip} dW_{pq} R_{qr} dW_{rs} R_{sj}\rangle &=& dt R_{ip}R_{qr}R_{sj}\left(\delta_{pr}\delta_{qs}+\delta_{ps}\delta_{qr}\right) \nonumber \\
&=& dt[R_{ip}R_{qp}R_{qj} + R_{ip}R_{qq}R_{pj}],
\end{eqnarray}
and can be written in an index free form, and crucially using $R=R^T$,  as
\begin{equation}
\langle R dW R dW R\rangle = dt[R^3 + R^2{\rm Tr}(R)].
\end{equation}
Writing $M$ in terms of $R$ we find the closed equation
\begin{eqnarray}
dR &=& R [-(z-R^{-1}) dt + dW] R + dt[R^3 + R^2{\rm Tr}(R)] \nonumber \\
&=& [-zR^2 + R + R^3 + R^2{\rm Tr}(R)] dt + d\Xi,
\end{eqnarray}
where $d\Xi = RdWR$ has the correlation function
\begin{eqnarray}
\langle d\Xi_{ij}d\Xi_{kl} \rangle &=& \langle R_{ip}dW_{pq} R_{qj} R_{kr}dW_{rs} R_{sl}\rangle
\nonumber \\
&=& dt \ R_{ip}R_{qj}R_{kr}R_{sl}\left[ \delta_{pr}\delta_{qs}+\delta_{ps}\delta_{qr}\right]
= dt [R_{ip}R_{qj}R_{kp}R_{ql} + R_{ip}R_{qj}R_{kq}R_{pl}]\nonumber  \\
&=& dt [R^2_{ik}R^2_{jl}  +  R^2_{il}R^2_{jk}].
\end{eqnarray}
Interestingly, in matrical terms we also have
\begin{equation}
\langle d\Xi d\Xi \rangle = R^4 + R^2 {\rm Tr} R^2.
\end{equation}
Taking the trace of the stochastic differential equation for $R$ gives
\begin{equation}
dr = [-z {\rm Tr}(R^2) + r + {\rm Tr}(R^3) + {\rm Tr}(R^2){\rm Tr}(R)] dt + d\xi,
\end{equation}
where $d\xi = {\rm Tr}( d\Xi)$ and so
\begin{equation}
\langle d\xi d\xi\rangle = dt \sum_{ik} R^2_{ik}R^2_{ik}  +  R^2_{ik}R^2_{ik} = 2dt  {\rm Tr} (R^4).
\end{equation}
We can close the above equation for $R$ as a function of $z$ by noting that
\begin{eqnarray}
{\rm Tr}(R^2) &=& -\frac{\partial r(z)}{\partial z}\nonumber \\
{\rm Tr}(R^3) &=& \frac{1}{2}\frac{\partial^2 r(z)}{\partial z^2} \nonumber \\
{\rm Tr}(R^4) &=&- \frac{1}{6}\frac{\partial^3 r(z)}{\partial z^3} .
\end{eqnarray}
Using this gives, in Langevin equation notation, 
\begin{equation}
\frac{\partial r(z,t)}{\partial t} = z\frac{\partial r(z,t)}{\partial z} + r(z,t) + \frac{1}{2}\frac{\partial^2 r(z,t)}{\partial z^2} - r(z,t) \frac{\partial r(z,t)}{\partial z} + \sqrt{- \frac{1}{3}\frac{\partial^3 r(z,t)}{\partial z^3}} \eta(t),
\end{equation}
where $\eta(t)$ is standard white noise with correlation function $\langle \eta(t)\eta(t')\rangle=\delta(t-t')$.

Now we anticipate that $r$ scales as $N$ and that the relevant variable for $N\times N$
matrices is given by $z=\zeta \sqrt{N}$ to write
\begin{equation}
r(z,t) = N \frac{1}{\sqrt N} f(\frac{z}{\sqrt{N}},t) = \sqrt{N} f(\zeta),
\end{equation}
from which we obtain
\begin{equation}
\frac{\partial f(\zeta,t)}{\partial t} = \zeta\frac{\partial f(\zeta,t)}{\partial \zeta} + f(\zeta,t) + \frac{1}{2N}\frac{\partial^2 f(\zeta,t)}{\partial \zeta^2} - f(\zeta,t) \frac{\partial f(\zeta,t)}{\partial \zeta} +\frac{1}{N} \sqrt{- \frac{1}{3}\frac{\partial^3 f(\zeta,t)}{\partial \zeta^3}} \eta(t),
\end{equation}
In the limit as $N\to\infty$ we keen the leading order terms to give
\begin{equation}
\frac{\partial f(\zeta,t)}{\partial t} = \zeta\frac{\partial f(\zeta,t)}{\partial \zeta} + f(\zeta,t)  - f(\zeta,t) \frac{\partial f(\zeta,t)}{\partial \zeta} .
\end{equation}
The steady state value for $f$ is thus given by
\begin{equation}
\zeta\frac{\partial f(\zeta)}{\partial \zeta} + f(\zeta)  - f(\zeta) \frac{\partial f(\zeta)}{\partial \zeta}=0 \label{bigf}.
\end{equation}
From the definition of $r(z)$ we see that as $|z|\to\infty$ we have
\begin{equation}
r(z) \sim \frac{N}{z},
\end{equation}
and so in the limit $\zeta\to\infty$ we have
\begin{equation}
f(\zeta) \sim \frac{N}{z\sqrt{N} } = \frac{1}{\zeta}.
\end{equation}
We notice that we can integrate Eq. (\ref{bigf}) once to obtain
\begin{equation}
\zeta f(\zeta) -\frac{1}{2} f^2(\zeta) = C,
\end{equation}
where $C$ is a constant. This gives
\begin{equation}
f(\zeta) = \zeta \pm \sqrt{\zeta^2-C},
\end{equation}
For large $\zeta$ we find 
\begin{equation}
f(\zeta) = \zeta(1 \pm \sqrt{1-\frac{C}{\zeta^2}}) \approx \zeta(1 \pm [1-\frac{C}{2\zeta^2}]).
\end{equation}
The boundary condition at large $|\zeta|$ then implies that we should take the negative root above and choose $C=2$, we thus find
\begin{equation}
f(\zeta) = \zeta - \sqrt{\zeta^2-2}.
\end{equation}
We see that $f(\zeta)$ only has an imaginary part when $\zeta^2 <2$ and so the density for the variables $\zeta$ is given by
\begin{equation}
\rho_*(\zeta) = \frac{1}{\pi}  \sqrt{2-\zeta^2},\ \zeta\in [-\sqrt{2}, \sqrt{2}],
\end{equation}
and where we have chosen $\sqrt{-1}=-i$ to have a positive density. This then gives
\begin{equation}
\rho(\lambda) = \frac{\sqrt{N}}{\pi}\sqrt{2-\frac{\lambda^2}{N}}= \frac{1}{\pi}\sqrt{2N-\lambda^2}.
\end{equation}
This density is called the Wigner-Semicircle law.
\chapter{Phase ordering kinetics}
\section{Introduction}
In this chapter we will analyse the dynamics of statistical systems. The analysis will allow us to understand how phase transitions occur dynamically. For instance we know that certain systems undergo what is known phase separation, for instance if we take the Ising model with zero magnetic field, in the high temperature phase the system is homogeneous and the average magnetisation, which is the order parameter for the transition is zero. Below the critical temperature if the overall magnetisation is conserved (for instance for Kawasaki dynamics), which would be the case if spins corresponded to different types of particles, the system will separate into two  phases of opposite average magnetisation, separated by an interface which will be roughly flat in order to minimise the surface energy between the two phases. For nonconserved systems, where the overall magnetisation in not conserved (for example Glauber dynamics), eventually one of the two phases will make up the system (spontaneous symmetry breaking). In a continuous phase transition as the critical point is reached from the disordered to ordered, domains of phases of positive and negative magnetisation form and the size of these domains is given by the correlation length of the system. For continuous phase transitions such as that in the Ising model the correlation length diverges as as the critical point is approached, for instance as $T\to T_c$ if the temperature is varied. The size of the domains thus have to become infinite if the system is infinite, this means that for an infinite system it will take an infinite time to relax to the equilibrium state. The process of domain growth is known as coarsening and phase ordering kinetics is the theory that has been developed to understand the phenomenon of coarsening. Furthermore, for systems with a conserved order parameter which separate into two phases, the two phases will be separated by an interface. This interface will be characterised by a surface tension, its average position will be fixed but it will exhibit fluctuations. Later we will see how model of phase ordering kinetics and be used to determine the static and dynamical properties of interfaces between two coexisiting phases. 

While the phase diagram of a system can be determined via 
its Hamiltonian and equilibrium statistical mechanics, the dynamics of coarsening depends on details of the systems dynamics that do not show up in single time thermodynamic observables. Therefore one needs to construct dynamical models that capture the underlying evolution of the state of the system, in particular there is a big difference between systems where the order parameter is conserved and those where it is not conserved.

\section{Statics of systems with a finite number of degrees of freedom}

Thermodynamic systems are naturally described in terms of fields, for example densities. This means that one is naturally lead to consider statistical field theories where the system is described in terms of a local field $\phi({\bf x})$. Statistical field theories can be applied to both statics, to understand phase diagrams, and dynamics to understand phase ordering. However to start with we will examine the case of systems with a finite number of degrees of freedom. 

Consider a system in the canonical ensemble with a Hamiltonian $H({\bf q})$ where $q_i$ for 
$1\leq i\leq N$ represent a finite number of continuous degrees of freedom. The partition function for the system is given by
\begin{equation}
Z = \int d{\bf q} \exp\left(-\beta H({\bf q})\right),
\end{equation}
and in equilibrium the probability density function $P_{eq}({\bf q})$ of the degrees of freedom is given by 
\begin{equation}
P_{eq}({\bf q}) = \frac{\exp\left(-\beta H({\bf q})\right)}{Z}.\label{eqdis}
\end{equation}
In general the integral which gives the  partition function cannot be computed analytically.
The simplest approximation to compute $Z$ is the mean field approximation where the integral 
is approximated by the integrand at its largest value - in mathematics this is the Laplace method for approximating an integral. The mean field approximation is thus
\begin{equation}
Z_{MF}= \exp\left(-\beta H({\bf q}^*)\right),
\end{equation}
where ${\bf q}^*$ is the value of ${\bf q}$ which minimises $H$ (note that the approximation becomes exact in the zero temperature limit - $\beta \to \infty$   - as the system will minimise its energy). The values $q_i^*$ are determined from
\begin{equation}
\frac{\partial H}{\partial q_i}|_{{\bf q}={\bf q^*}}=0.
\end{equation}
Within this approximation any thermodynamic observable is given by
\begin{equation}
\langle f({\bf q}) \rangle = f({\bf q}^*).
\end{equation}

We now consider how one can model dynamics of such systems. We will look for a Langevin equation which is chosen to give the correct equilibrium Gibbs-Boltzmann distribution. We write
\begin{equation}
\frac{d q_i}{dt} = -L_{ij}\frac{\partial H({\bf q})}{ \partial q_j} + \eta_i(t),
\end{equation}
where $L_{ij}$ is a matrix which discuss later and $\eta_i(t)$ is zero mean Gaussian white noise  with correlation function 
\begin{equation}
\langle \eta_i(t)\eta_j(t')\rangle =  \Gamma_{ij} \delta(t-t'),\label{cfn}
\end{equation}
The Gaussian white noise represents the effects of thermal fluctuations on the system we assume that the correlation time of these fluctuations is extremely short with respect to the dynamics of the degrees of freedom $q_i$ (in fact in critical systems the dynamics becomes very slow, critical slowing down, and this approximation becomes better and better as one approaches the critical point).  As Eq. (\ref{cfn}) is for a correlation function the matrix $\Gamma_{ij}$ must be symmetric and cannot have any negative eigenvalues.

In the absence of noise or thermal fluctuations, so at zero temperature, the system will simply minimise its energy. Therefore if 
\begin{equation}
\frac{\partial H({\bf q})}{ \partial q_j} =0, 
\end{equation}
with no noise we have $\frac{d q_i}{dt}=0$, that is to say it is the term $\frac{\partial H({\bf q})}{ \partial q_j}$ that drives the dynamics if there is no noise. As long as the matrix $L_{ij}^{-1}$ exists the zero temperature dynamics will take the system to the local minimum of $H$ and to the absolute minimum if there are no metastable configurations. 

Under these assumptions, the Fokker-Planck equation for the probability density function of the degrees of freedom is 
\begin{equation}
\frac{\partial p({\bf q},t)}{\partial t} = \frac{\partial}{\partial q_i} \left[\frac{1}{2}\Gamma_{ij} \frac{\partial p({\bf q},t)}{\partial q_i} + p({\bf q},t) L_{ij}\frac{\partial H({\bf q})}{ \partial q_j}\right].
\end{equation}
This can be written as 
\begin{equation}
\frac{\partial p({\bf q},t)}{\partial t} +\frac{\partial}{\partial q_i}J_i({\bf q},t)=0,
\end{equation}
where the ${\bf J}({\bf q},t)$ is the probability current. We now insist that the system is in equilibrium with zero current when $p({\bf q},t)= P_{eq}({\bf q})$ as given by Eq. (\ref{eqdis}), this gives
\begin{equation}
\left[-\frac{\beta}{2}\Gamma_{ij} + L_{ij}\right]\frac{\partial H({\bf q})}{ \partial q_j},
\end{equation}
and this holds for any choice of $H$ is we chose.
\begin{equation}
\Gamma_{ij}= 2T L_{ij}
\end{equation}
where we have taken units where Boltzmann's constant $k_B=1$. 
\section{Statistical field theory}
We now consider a system with Hamiltonian $H[\phi]$ which depends on a continuous field 
$\phi({\bf x})$. The partition function is given by a functional integral
\begin{equation}
Z = \int d[\phi] \exp(-\beta H[\phi]),
\end{equation}
the functional integral over all possible fields $\phi$ can be taken as a limit where $\phi$ is defined at a finite number of points on a lattice and then the lattice spacing is taken to zero. 

The mean field approximation to partition function is then given by
\begin{equation}
Z _{MF}=  \exp(-\beta H[\phi_{MF}]),
\end{equation} 
where $\phi_{MF}$ is the mean field solution which minimises $H$. The definition of a functional derivative of a functional is
\begin{equation}
F[\phi+\delta\phi]-F[\phi]= \int d{\bf x} \frac{\delta F}{\delta\phi({\bf x})} \delta\phi({\bf x}).
\end{equation}
Therefore if a field $\phi$ maximises $H$ we must have 
\begin{equation}
\frac{\delta H}{\delta\phi({\bf x})}=0.
\end{equation}

We now consider the standard Landau-Ginzburg Hamiltonian describing Ising like systems where
\begin{equation}
H[\phi] = \int d{\bf x} \ \frac{\kappa}{2}[\nabla \phi]^2 + V(\phi) .
\end{equation}
The first term represents an energetic cost of varying the field $\phi$ while the second potential term has two minima at $\phi=\pm \phi_c$, and without loss of generality we can chose  $V(\phi_c)=V(-\phi_c)$, in the low temperature or phase separated phase and a single minimum at $\phi=0$ in the high temperature phase. The standard, so called, $\phi^4$ form is
\begin{equation}
V(\phi) = \frac{1}{2} m^2 \phi^2 + \frac{\lambda}{4!} \phi^4,\label{p4}
\end{equation} 
where 
\begin{equation}
m^2 = T-T_c.
\end{equation}
It is easy to see that 
\begin{equation}
\frac{\delta H}{\delta \phi({\bf x})} = -\kappa \nabla^2 \phi({\bf x}) + V'(\phi).\label{cm}
\end{equation}
If there is non constraint on the system if can simply chose $\phi({\bf x}) =\phi_c$ or $\phi({\bf x}) =-\phi_c$ everywhere which corresponds to a  free energy $F=H[\phi_c]=0$. However in a system with a conserved order parameter
\begin{equation}
\int d{\bf x} \  \phi({\bf x})=0, 
\end{equation}
then the solutions $\phi=\pm \phi_c$ cannot hold. In this case the system will separate into a two phases where $\phi({\bf x})= \pm \phi_c$. We therefore choose an interface at $z=0$ where 
and take $\phi({\bf x}) = \phi_K(z)$ ($K$ standing for kink as it is known as the kink solution in the literature) where $\lim_{z\to\-\infty}=-\phi_c$ and  $\lim_{z\to\infty}=-\phi_c$. 
We therefore find from Eq. (\ref{cm}) that
\begin{equation}
-\kappa \frac{d^2 }{dz^2}\phi_K(z)  + V'(\phi_K) = 0 \label{kk0}
\end{equation}
This equation can be solved for the potential in \eqref{p4} ({\em you should do it and fill in the details}) but even without knowing the explicit solution we can write
\begin{equation}
H[\phi_K]=  A\int dz \ \frac{\kappa}{2}\left(\frac{d\phi_K(z)}{dz}\right)^2 + V(\phi_K(z)),\label{kk1}
\end{equation}
where $A$ is the surface area of the system in the plane perpendicular to the direction $z$. 
However if we multiply Eq. \eqref{kk0} by $d\phi/dz$ and integrate we find
\begin{equation}
-\frac{\kappa}{2} (\frac{d\phi_K}{dz})^2 + V(\phi_K) = C,
\end{equation}
where $C$ is a constant. However as $\phi_K(z)\to \pm \phi_c$ as $z\to \pm \infty$ and $V(\pm\phi_c) =0$ we find that $C=0$. Using this we obtain 
\begin{equation}
H[\phi_K]=  A\int dz\  {\kappa}\left(\frac{d\phi_K(z)}{dz}\right)^2 .
\end{equation}
If the interface has a free energy per unit area of $\sigma$ then we have the Cahn-Hillard estimate of the surface tension 
\begin{equation}
\sigma=  \int dz\  {\kappa}\left(\frac{d\phi_K(z)}{dz}\right)^2 .\label{CHST}
\end{equation}

Now we return to dynamics. If we compare with systems with a discrete number of variables we
should have a Langevin equation of the form
\begin{equation}
\frac{\partial \phi({\bf x})}{\partial t}= -L \frac{\delta H}{\delta \phi({\bf x})} + \eta({\bf x},t).
\end{equation}
The white noise correlator should have the form
\begin{equation}
\langle \eta({\bf x},t)\eta({\bf x}',t)\rangle =\delta(t-t')\Gamma({\bf x},{\bf x'}),
\end{equation}
where here  $\Gamma({\bf x},{\bf x'})$ is an operator (before it was a matrix) defined by its action on functions $f$ as
\begin{equation}
\Gamma f({\bf x}) = \int d{\bf x}' \Gamma({\bf x},{\bf x}')f({\bf x}'),
\end{equation}
and $L$ is also an operator with 
\begin{equation}
L f({\bf x}) = \int d{\bf x}' L({\bf x},{\bf x}')f({\bf x}'),
\end{equation}
Following the same arguments for systems with a finite number of degrees of freedom we thus have the relation (which is sometimes called the fluctuation dissipation theorem as it essentially is equivalent)
\begin{equation} 
\Gamma({\bf x},{\bf x}') =2T L({\bf x},{\bf x}').
\end{equation}
The simplest form of dynamics is given by $L({\bf x},{\bf x}')=\alpha\delta({\bf x}-{\bf x}')$ which gives the model A dynamics
\begin{equation}
\frac{\partial \phi({\bf x})}{\partial t}= -\alpha \frac{\delta H}{\delta \phi({\bf x})} + \eta({\bf x},t),\label{MA}
\end{equation}
with the noise correlator
\begin{equation}
\langle \eta({\bf x},t)\eta({\bf x}',t)\rangle =2T \alpha \delta(t-t')\delta({\bf x}-{\bf x'}).
\end{equation}
The average value of $\phi$ 
\begin{equation}
\overline \phi(t) = \frac{1}{V}\int d{\bf x}\  \phi({\bf x},t),
\end{equation}
is clearly not generally conserved by this dynamics.

Model $B$ dynamics amounts to choosing
\begin{equation}
L({\bf x}-{\bf x}')= -D\nabla^2 \delta({\bf x}-{\bf x'}),
\end{equation}
here the fact that $L$ is a positive semi-definite operator can be seen by taking its Fourier transform. The evolution equation here is
\begin{equation}
\frac{\partial \phi({\bf x})}{\partial t}= D\nabla^2 \frac{\delta H}{\delta \phi({\bf x})} + \eta({\bf x},t),
\label{MB}
\end{equation}
and where
\begin{equation}
\langle \eta({\bf x},t)\eta({\bf x}',t)\rangle =-2TD   \delta(t-t')\nabla^2\delta({\bf x}-{\bf x'}).
\end{equation}
We notice that if we introduce the vectorial white noise with components $\eta_i({\bf x},t)$ such that
\begin{equation}
\langle \eta_i({\bf x},t) \eta_i({\bf x}',t')\rangle =\delta_{ij} \delta({\bf x}-{\bf x'})\delta(t-t),
\end{equation}
where $\delta_{ij}=1$ for $i=j$ and is zero otherwise,  we can write
\begin{equation}
\eta({\bf x},t)= \nabla\cdot {\boldsymbol \eta}({\bf x},t),
\end{equation}
as one can verify the two noises have the same correlation function. In this way Eq. (\ref{MB}) becomes 
\begin{equation}
\frac{\partial \phi({\bf x})}{\partial t}= \nabla\cdot[ D\nabla \frac{\delta H}{\delta \phi({\bf x})} + {\boldsymbol\eta}({\bf x},t)].
\end{equation}
From this it is easy to see that the order parameter is conserved - thus model  B describes conserved phase ordering dynamics.

\section{Models for equilibrium interfaces}
Here we discuss effective models of interfaces. The simplest model is to assume that the 
interface is parameterised by a height profile $h({\bf r})$, however one also has to assume that 
$h({\bf x})$ is a single valued function of ${\bf r}$. Given this one can write
\begin{equation}
H[h] = \sigma A[h]
\end{equation}
where $A_h$ is the area of the interface. However the interface area is given by
\begin{equation}
A[h] = \int_A d{\bf r}\sqrt{1+[\nabla h]^2},
\end{equation}
where the integral is over the plane perpendicular to the $z$ axis which is taken to be of area $A$. When the fluctuations of the interface are small, we can expand the above to quadratic order in $h$ to obtain
\begin{equation}
H[h]= A\sigma +\frac{\sigma}{2} \int_A d{\bf r} \ [\nabla h]^2.
\end{equation}
The first term is independent of the height so we can write the effective Hamiltonian for the surface as
\begin{equation}
H_{eff} [h]= \frac{\sigma}{2} \int_A d{\bf r}\  [\nabla h]^2.\label{heff}
\end{equation}
\section{Effective dynamics of interface heights}
We will now try and derive an approximation for the dynamics of the  height of the interface from the original phase ordering kinetics. We imagine that the system is phase separated in the direction  $z$, on average the interface is taken to be at $z=0$, and we write
\begin{equation}
\phi(z,{\bf r},t) = f(z-h({\bf r},t))\label{hans}
\end{equation}
where $f(z)=\phi_K(z)$ is the kink solution from mean field theory.
\\

\noindent{\bf Model A dynamics}

substitute Eq. \eqref{hans} into Eq. \eqref{MA} and make use of the following results
\begin{eqnarray}
\frac{\partial f(z-h({\bf r},t))}{\partial t}&=& -f'(z-h({\bf r},t))\frac{\partial h({\bf r},t)}{\partial t}\\
\nabla f(z-h({\bf r},t))&=& [{\bf e}_z-\nabla h({\bf r},t)]f'(z-h({\bf r},t))  \\
\nabla^2 f(z-h({\bf r},t))&=& f''(z-h({\bf r},t))- \nabla^2 h({\bf r},t)f'(z-h({\bf r},t))+ [\nabla h({\bf r},t)]^2 
f''(z-h({\bf r},t))
\end{eqnarray}
and thus find
\begin{eqnarray}
&&-f'(z-h({\bf r},t))\frac{\partial h({\bf r},t)}{\partial t}= \alpha\kappa\times\\ &&\left[f''(z-h({\bf r},t))- \nabla^2 h({\bf r},t)f'(z-h({\bf r},t))+ [\nabla h({\bf r},t)]^2 
f''(z-h({\bf r},t))\right] - \alpha V'(f'(z-h({\bf r},t))) + \eta({\bf r},z,t).
\end{eqnarray}
We now multiply both sides of this equation by $f'(z-h({\bf r},t))$ and defining $\zeta=z-h({\bf r},t)$ we integrate  $\zeta$ over $[-\infty,\infty]$ and use the following identities
\begin{eqnarray}
\int_{-\infty}^\infty d\zeta f'(\zeta)f''(\zeta) &=& [\frac{1}{2}f'^2(\zeta)]_{-\infty}^\infty =0\\
\int_{-\infty}^\infty d\zeta f'(\zeta) V'(f) &=& \int_{-\infty}^\infty d\zeta\frac{d V(f)}{d\zeta}= [V(f(\zeta))]_{-\infty}^\infty=0,
\end{eqnarray} 
note that the first relation above holds as $f(\zeta)=\pm \phi_c$ as $\zeta\to\pm \infty$ and the second as
$V(\phi_c)=V(-\phi_c)=0$.
The terms that are left then give
\begin{equation}
-\int_{-\infty}^\infty f'^2(\zeta)d\zeta\ \frac{\partial h({\bf r},t)}{\partial t}
= -\alpha\int_{-\infty}^\infty f'^2(\zeta)d\zeta \ \kappa \nabla^2 h({\bf r},t) + \int_{-\infty}^\infty d\zeta \eta({\bf r},\zeta+ h({\bf r},t))f'(\zeta)
\end{equation}
Now using the Cahn-Hillard estimate of the surface tension Eq. \eqref{CHST} this becomes
\begin{equation}
\frac{\sigma}{\kappa} \frac{\partial h({\bf r},t)}{\partial t}
= \alpha\sigma \nabla^2 h({\bf r},t) +\xi({\bf r},t),
\end{equation}
where the noise term is given by
\begin{equation}
\xi({\bf r},t)= \int_{-\infty}^\infty d\zeta \eta({\bf r},\zeta+ h({\bf r},t))f'(\zeta)
\end{equation}
The noise term has zero mean and correlation function
\begin{eqnarray}
\langle \xi({\bf r},t)\xi({\bf r}',t')\rangle &=&2\alpha T\delta(t-t')\delta({\bf r}-{\bf r}')\int_{-\infty}^\infty d\zeta d\zeta' \delta(\zeta-\zeta')f'(\zeta)f'(\zeta')\\
&=& 2\alpha T\delta(t-t')\delta({\bf r}-{\bf r}')\int_{-\infty}^\infty d\zeta f'^2(\zeta)= \frac{2\alpha T\sigma}{\kappa}\delta(t-t')\delta({\bf r}-{\bf r}').
\end{eqnarray}
This now gives
\begin{equation}
\frac{\partial h({\bf r},t)}{\partial t}= \kappa\alpha \nabla^2 h({\bf r},t) + \eta({\bf r},t)
\end{equation}
where 
\begin{equation}
\langle \eta({\bf r},t)\eta({\bf r}',t')\rangle = \frac{2\alpha T\kappa}{\sigma}\delta(t-t')\delta({\bf r}-{\bf r}').
\end{equation}
Now defining $\alpha' = \frac{\kappa\alpha}{\sigma}$ we can write
\begin{equation}
\frac{\partial h({\bf r},t)}{\partial t}= \alpha' \sigma\nabla^2 h({\bf r},t) + \eta({\bf r},t).
\end{equation}
This has the form of model $A$ dynamics for the height profile with Hamiltonian
$H_{eff}$ as given in \eqref{heff}, that is to say we can write
\begin{equation}
\frac{\partial h({\bf r},t)}{\partial t}= -\alpha' \frac{\delta H_{eff}[h]}{\delta h({\bf r})} + \eta({\bf r},t),
\label{ew}
\end{equation}
and where 
\begin{equation}
\langle \eta({\bf r},t)\eta({\bf r}',t')\rangle= 2T\alpha'\delta(t-t').
\end{equation}
This dynamical calculation is thus consistent with the idea of describing the surface in terms of a height variable with an energy given by the surface tension. The equation \eqref{ew} is known as the Edwards-Wilkinson equation. We can use this equation to determine how the domains of a coarsening systems grow at low temperatures. To do this we ignore the noise term and assume that at $t=0$ the correlations of the height are short range so
\begin{equation}
C({\bf r}-{\bf r}',0)= \langle h({\bf r},0)h({\bf r}',0)\rangle =C_0 \delta({\bf r}-{\bf r}').
\end{equation}
In Fourier space the noiseless Edwards-Wilkinson equation becomes
\begin{equation}
\frac{\partial\tilde h({\bf k},t)}{\partial t} = -\alpha'\sigma \tilde h({\bf k},t) ,
\end{equation}
and so we find
\begin{equation}
\tilde h({\bf k},t) = h({\bf k},0)\exp(-\alpha'\sigma{\bf k}^2 t).
\end{equation}
We thus find 
\begin{equation}
\langle \tilde h({\bf k},t)\tilde h({\bf k}',t')\rangle = \langle h({\bf k},0)h({\bf k}',0)\rangle \exp(-\alpha'\sigma[k^2+k'^2] t).
\end{equation}
Now recall that if 
\begin{equation}
\langle  h({\bf r},t) h({\bf r}',t')\rangle =C({\bf r}-{\bf r}',t)
\end{equation}
then
\begin{equation}
\langle \tilde h({\bf k},t)\tilde h({\bf k}',t')\rangle= (2\pi)^d \delta({\bf k}+{\bf k}') \tilde C({\bf k},t),
\end{equation}
where 
\begin{equation}
\tilde C({\bf k},t)= \int d{\bf r} \exp(-i{\bf k}\cdot {\bf r})C({\bf r},t),
\end{equation}
is the Fourier transform of the correlation function which is a function of a single position due to invariance by translation in space, and $d$ is the dimension of space (so here $d=2$ for a surface in 3d space and $d=1$ for a surface in a 2d space). Putting all this together gives
\begin{equation}
\tilde C({\bf k},t)= C_0 \exp(-2\alpha'\sigma k^2 t).
\end{equation}
Inverting the Fourier transform gives
\begin{equation}
C({\bf r},t)= \frac{C_0}{(8\pi \alpha'\sigma t)^{\frac{d}{2}}} \exp(-\frac{{\bf r}^2}{16\pi \alpha'\sigma t}).
\end{equation}
From this we see that if $C({\bf r},t)\sim g(\frac{{\bf r}}{\ell(t)})r(t)$ then the length scale $\ell(t)\sim t^{\frac{1}{2}}$, this agrees with what we found in the Ising model under Glauber dynamics, where the growth exponent is also given by $z=\frac{1}{2}$.
\\

\noindent{\bf Model B dynamics}

Here we take the same ansatz as in Eq. \eqref{hans} but we rewrite the model B dynamics as
\begin{equation}
-\nabla^{-2} \frac{\partial\phi({\bf x},t)}{\partial t} = -D \frac{\delta H}{\delta \phi({\bf x})}+ 
\theta({\bf x},t),
\end{equation}
here $-\nabla^{-2}$ represents the Green's function $G$ which obeys 
\begin{equation}
\nabla^2 G({\bf x}-{\bf x'}) = -\delta({\bf x}-{\bf x}'),
\end{equation}
and 
\begin{equation}
\theta({\bf x},t)=-\nabla^{-2}\eta({\bf x},t)= \int d{\bf x}'G({\bf x}-{\bf x'})\eta({\bf x},t)
\end{equation}
The correlation function of $\theta({\bf x},t)$ is given by
\begin{eqnarray}
\langle \theta({\bf x},t)\theta({\bf y},t')\rangle &=&-2DT\delta(t-t') \int d{\bf x}'G({\bf x}-{\bf x'})d{\bf y}'G({\bf y}-{\bf y'})\nabla^2\delta({\bf x}'-{\bf y}') \\
&=& -2DT\delta(t-t') \int d{\bf x}'G({\bf x}-{\bf x'})d{\bf y}'\nabla^2G({\bf y}-{\bf y'})\delta({\bf x}'-{\bf y}') \\
&=& 2DT\delta(t-t') G({\bf x}-{\bf y}).
\end{eqnarray}
where we have integrated by parts in the second line and used 
\begin{equation}
-\nabla^2G({\bf y}-{\bf y'})= \delta({\bf y}-{\bf y}'),
\end{equation}
in the third.

Now mutliplying by $f'(z-h({\bf r},t))$ and integrating $z$ over $[-\infty,\infty]$, we find
\begin{eqnarray}
&&-\int dz f'(z-h({\bf r},t))\int dz'd{\bf r}'\  G(z-z',{\bf r}-{\bf r}') f'(z'-h({\bf r}',t))\frac{\partial h({\bf r}',t)}{\partial t} = \\
&&-D\sigma \nabla^2 h({\bf r},t) + \chi({\bf r},t),
\end{eqnarray}
with the noise
\begin{equation}
\chi({\bf r},t)= \int dz f'(z-h({\bf r},t)) \theta({\bf r},z,t).
\end{equation}
As we assume that the height fluctuations are small we keep only the lowest order terms in $h$ in the deterministic terms and the noise, we will see later that this is compatible thermodynamically. We thus have
\begin{eqnarray}
&&-\int dz \ f'(z)\int dz'd{\bf r}'\  G(z-z',{\bf r}-{\bf r}') f'(z')\frac{\partial h({\bf r}',t)}{\partial t} = \\
&&-D\sigma \nabla^2 h({\bf r},t) + \chi({\bf r},t),
\end{eqnarray}
and now  the noise is given by
\begin{equation}
\chi({\bf r},t)= \int dz\  f'(z) \theta({\bf r},z',t).
\end{equation}
This equation which is linear in $h$ can now be Fourier transformed in the plane ${\bf r}$ and in terms of the Fourier transform of $h$ we find
\begin{equation}
-\int dz \ f'(z)\int dz'd{\bf r}'\ \tilde G(z-z',{\bf k}) f'(z')\frac{\partial\tilde h({\bf k},t)}{\partial t} = 
Dk^2\sigma \tilde h({\bf k},t) + \tilde \chi({\bf k},t).\label{bstep}
\end{equation}
The Fourier transform of $G$ in the ${\bf r}$ plane obeys
\begin{equation}
\frac{d^2 \tilde G(z-z',{\bf k} )}{dz^2}-k^2 \tilde G(z-z',{\bf k} )=-\delta(z-z')
\end{equation}
and the solution to this equation (with the boundary condition that $\tilde G(z-z',{\bf k} )\to 0$ as $|z-z'|\to\infty$)  is
\begin{equation}
\tilde G(z-z',{\bf k}) = \frac{\exp(-k|z-z'|)}{2k},
\end{equation}
and note that $k=|{\bf k}|$. 
Next we make the sharp interface approximation where we write
\begin{equation}
f(z) = 2\phi_c \delta(z),\label{sharp}
\end{equation}
that is to say we have replaced the smooth kink solution with a step like solution
$f(z) = \phi_c\  {\rm sgn}(z)$. This then gives
\begin{equation}
-4\phi_c^2 \tilde G(0,k) \frac{\partial h({\bf k},t)}{\partial t} = 
Dk^2\sigma  h({\bf r},t) + \tilde \chi({\bf k},t),
\end{equation}
which we rewrite as
\begin{equation}
\frac{\partial \tilde h({\bf k},t)}{\partial t} = -\frac{Dk^3\sigma}{2\phi_c^2} \tilde h({\bf k},t) + \tilde \xi({\bf k},t),
\end{equation}
with 
\begin{equation}
\tilde \xi({\bf k},t)= - \frac{k}{2\phi_c^2}\tilde \chi({\bf k},t),
\end{equation}
where 
\begin{equation}
\tilde \chi({\bf k},t)= \int dz\  f'(z) \tilde\theta({\bf k},z,t),
\end{equation}
The correlation function of $\tilde \theta({\bf k},t)$ is 
\begin{equation}
\langle \theta({\bf k},t)\theta({\bf k}',t')\rangle = 2DT(2\pi)^d \delta(t-t') \delta({\bf k}+{\bf k'}) \tilde G(z-z',k)
\end{equation}
and from this we find 
\begin{equation}
\langle \chi({\bf k},t)\chi({\bf k}',t')\rangle  = 2DT(2\pi)^d \delta(t-t') \delta({\bf k}+{\bf k'}) \int dz dz'
f(z) f(z') \tilde G(z-z',k),\label{bstep2}
\end{equation}
now using the sharp interface approximation Eq. (\ref{sharp}) we obtain
\begin{equation}
\langle \chi({\bf k},t)\chi({\bf k}',t')\rangle  = 2DT(2\pi)^d \delta(t-t') \delta({\bf k}+{\bf k'}) \frac{2\phi_c^2}{k},
\end{equation}
and consequently
\begin{equation}
\langle \xi({\bf k},t)\xi({\bf k}',t')\rangle = 2DT(2\pi)^d \delta(t-t') \delta({\bf k}+{\bf k'}) \frac{k}{2\phi_c^2}.
\end{equation}
Finally in we find the interface dynamics for model B in Fourier space is
\begin{equation}
\frac{\partial h({\bf k},t)}{\partial t} = -\frac{Dk^3\sigma}{2\phi_c^2} \tilde h({\bf k},t) + \tilde \xi({\bf k},t),\label{modBFT}
\end{equation}
In real space this has the form
\begin{equation}
\frac{\partial h({\bf x})}{\partial t}= -L \frac{\delta H_{eff}}{\delta h ({\bf x})} + \xi({\bf x},t).
\end{equation}
where the operator $L$ is defined via its Fourier transform
\begin{equation}
\tilde L({\bf k}) = \frac{Dk}{2\phi_c^2}.
\end{equation}
Now if we look at Eq. \eqref{modBFT} we see that solving the equation without noise will give a function of $k^3t$, which in real space corresponds to $x^3/t$. From this we see that the coarsening length scale grows as $\ell(t) \sim t^{\frac{1}{3}}$ and consequently the coarsening exponent is $z=\frac{1}{3}$.  Coarsening for conserved model B or diffusive dynamics is slower than that of model A. One of the reasons for this slowing down with respect to nonconserved dynamics is that material must be physically transported by diffusion (by exchanging spins in the language of lattice spin models), where as for  model A dynamics the composition can change at any given point by {\em spin flipping}.

We can actually do better than the above sharp interface approximation as Eq. \eqref{bstep} can be written as
\begin{equation}
Q(k)\frac{\partial\tilde h({\bf k},t)}{\partial t} = 
-Dk^2\sigma \tilde h({\bf k},t) - \tilde \chi({\bf k},t).\label{bstep2}
\end{equation}
where 
\begin{equation}
Q(k)= \int dzdz' \ f'(z)\ \tilde G(z-z',{\bf k}) f'(z').
\end{equation}
Notice that from Eq. \eqref{bstep2} that
\begin{equation}
\langle \chi({\bf k},t)\chi({\bf k}',t')\rangle  = 2DT(2\pi)^d \delta(t-t') \delta({\bf k}+{\bf k'}) Q(k).\end{equation}
and so 
\begin{equation}
\frac{\partial\tilde h({\bf k},t)}{\partial t}= -\tilde L(k) \tilde \mu({\bf k}) + \eta({\bf k}),
\end{equation}
where $\mu(x)=\delta H_{eff}/\delta h({\bf x}) $ and $\tilde L(k) = D/Q(k)$ and 
\begin{equation}
\langle \eta({\bf k},t)\eta({\bf k}',t')\rangle  = 2T(2\pi)^d \delta(t-t') \delta({\bf k}+{\bf k'}) \tilde L(k).\end{equation}
\end{document}




















































\section{The Rouse chain polymer in a hydrodynamic flow}

\begin{equation}
H_{el}= \frac{\kappa}{2}\sum_{i=1}^{N-1} (x_{i+1}-x_{i})^2
\end{equation}

\begin{equation}
\gamma(d{\bf X}_i(t)-{\bf v}({\bf X}_i(t))dt) = -\frac{\partial H({\bf X})}{\partial x_i} dt + \sqrt{2T\gamma} d{\bf B}_i
\end{equation}

\begin{equation}
d{\bf X}_i(t) = {\bf v}({\bf X}_i(t))dt-\frac{1}{\gamma}\frac{\partial H({\bf X})}{\partial x_i} dt + \sqrt{\frac{2T}{\gamma}} d{\bf B}_i.
\end{equation}

\begin{equation}
{\bf v}({\bf x}) = \nabla \phi({\bf x})
\end{equation}

\begin{equation}
\phi({\bf x}) = \frac{1}{2} {\bf x}\cdot A {\bf x}
\end{equation}

\begin{equation}
{\bf v}({\bf x}) = A{\bf x}
\end{equation}

\begin{equation}
\nabla \cdot{\bf v} =\sum_{ij} \frac{\partial}{\partial x_i}A_{ij}x_j = \sum_{i} A_{ii} = {\rm Tr}(A) = 0.
\end{equation}

\begin{equation}
d{\bf X}_i(t) = A{\bf X}_i(t)dt-\frac{1}{\gamma}\frac{\partial H({\bf X})}{\partial x_i} dt + \sqrt{\frac{2T}{\gamma}} d{\bf B}_i.
\end{equation}

\begin{equation}
{\bf X}_c(t) = \frac{1}{N}\sum_i {\bf X}_i(t)
\end{equation}

\begin{equation}
{\bf Y}_i(t) = {\bf X}_i(t) - {\bf X}_c(t) 
\end{equation}

\begin{equation}
H({\bf X})= H({\bf Y})
\end{equation}

\begin{equation}
d{\bf X}_c(t) = A{\bf X}_c(t)dt + \frac{1}{N}\sqrt{\frac{2T}{\gamma}} \sum d{\bf B}_i.
\end{equation}
\begin{equation}
d{\bf Y}_i(t) = A{\bf Y}_i(t)dt-\frac{1}{\gamma}\frac{\partial H({\bf Y})}{\partial y_i} dt + \sqrt{\frac{2T}{\gamma}} d{\bf B}_i- \frac{1}{N}\sqrt{\frac{2T}{\gamma}} \sum d{\bf B}_i.
\end{equation}

\begin{equation}
d{\bf Y}_i(t) = A{\bf Y}_i(t)dt-\frac{1}{\gamma}\frac{\partial H({\bf Y})}{\partial y_i} dt + \sqrt{\frac{2T}{\gamma}} d{\bf W}_i
\end{equation}

\begin{equation}
\langle d{\bf W}_i d{\bf W}_j^T\rangle = I (\delta_{ij}-\frac{1}{N})
\end{equation}

 \end{document} 
 \section{Correlation and response functions via the Fokker-Planck equation}
 
Consider a system of stochastic differential equations describing the dynamical variables ${\bf X}_t$, in general ${\bf X}_t$ represent points in phase space such as positions and velocities. 
The  correlation function between two observables at two different times $t$ and $t'$ is defined via
 \begin{equation}
 \langle A({\bf X}_t) B({\bf X}_t')\rangle.
 \end{equation}
 Consider a system which is  in equilibrium at $t=0$ with equilibrium probability distribution function $p_{eq}({\bf x}_0)$. By definition the equilibrium probability density function obeys the time independent Fokker-Planck equation
 \begin{equation}
 G^\dagger p_{eq}({\bf x}) = 0.
 \end{equation}
 
The joint probability distribution function that  it is at  ${\bf x}_0$ at time $0$ and then ${\bf x}'$ at time $t'$ is given by
 \begin{equation}
 \rho({\bf x}',t'; {\bf x_0}, 0) = p({\bf x}',{\bf x}_0,t')p_{eq}({\bf x}_0).
 \end{equation}
 This is the Chapman-Kolmogorov formula, which is valid for Markov processes - these are processes whose future evolution only depends on the current state of the system and not its whole history. Similarly
 the probability density to be at ${\bf x}_0$ at time $0$, ${\bf x}'$ at time $t'$ and then ${\bf x}$ at time $t$ is given by
 \begin{equation}
 \rho({\bf x},t; {\bf x}',t'; {\bf x_0}, 0) = p({\bf x},{\bf x}',t-t')p({\bf x}',{\bf x}_0,t')p_{eq}({\bf x}_0).
 \end{equation}
 Note that if the system is in equilibrium at time $0$, then it should still be in equilibrium at time $t$, this means that 
 \begin{equation}
 p({\bf x},t) = \int d{\bf x}_0 \rho({\bf x},t; {\bf x_0}, 0) = p_{eq}({\bf x}),
 \end{equation}
to check this we use Eq. \eqref{schrod} to write
\begin{equation}
\rho({\bf x}',t'; {\bf x_0}, 0)= \exp(tG_{\bf x}^\dagger)\delta({\bf x}-{\bf x}_0)p_{eq}({\bf x}_0),
\end{equation}
 where we have used the notation $G^\dagger_{\bf x}$ to precise that the operator $G^\dagger$ acts on the coordinates ${\bf x}$ (and not ${\bf x}_0$). Using this we find (the delta function converts all coordinates ${\bf x}_0$ to ${\bf x}$)
 \begin{equation}
p({\bf x},t)= \exp(tG_{\bf x}^\dagger)p_{eq}({\bf x}),
 \end{equation}
 however as $G_{\bf x}^\dagger p_{eq}({\bf x})=0$, we find
\begin{equation}
p({\bf x},t)= p_{eq}({\bf x}),
\end{equation}
so once the system is in equilibrium it will stay in equilibrium. Using this  correlation function is given by
\begin{eqnarray}
\langle A({\bf X}_t) B({\bf X}_{t'})\rangle &=& \int d{\bf x}d{\bf x}' d{\bf x}_0 
\rho({\bf x},t; {\bf x}',t'; {\bf x_0}, 0)  A({\bf x}) B({\bf x'})\\
&=& \int d{\bf x}d{\bf x}' d{\bf x}_0 p({\bf x},{\bf x}',t-t')p({\bf x}',{\bf x}_0,t')p_{eq}({\bf x}_0)
  A({\bf x}) B({\bf x'})\\
  &=& \int d{\bf x}d{\bf x}' d{\bf x}_0 p({\bf x},{\bf x}',t-t')p_{eq}({\bf x}')
  A({\bf x}) B({\bf x'}) ,
\end{eqnarray}
and this can be written as
\begin{equation}
\langle A({\bf X}_t) B({\bf X}_{t'})\rangle
  = \langle A({\bf X}_{t-t'}) B({\bf X}_0)\rangle.\label{tti1}
\end{equation}
This is simply the property of time translational invariance of the equilibrium state which takes the form
\begin{equation}
\langle A({\bf X}_{t+\tau}) B({\bf X}_{t'+\tau })\rangle = \langle A({\bf X}_{t}) B({\bf X}_{t'})\rangle,
\end{equation}
for any $\tau$, choosing $\tau= -t'$ gives the result Eq. \eqref{tti1}.

\begin{equation}
\langle A({\bf X}_t)\rangle = \int d{\bf x}d{\bf x'}d{\bf x_0} A({\bf x}) 
p({\bf x},{\bf x'}, t-\Delta t) p_{B}({\bf x}',{\bf x}_0, \Delta t)p_{eq}({\bf x}_0)
\end{equation}

\begin{equation}
p({\bf x},{\bf x'}, t) =\exp(t G_{\bf x}^\dagger)\delta({\bf x}-{\bf x}')
\end{equation}

\begin{equation}
p_B({\bf x},{\bf x'}, t) =\exp(t [G_{\bf x}^\dagger +\epsilon\Delta G_{\bf x}^\dagger])\delta({\bf x}-{\bf x}')
\end{equation}

\begin{eqnarray}
\langle A({\bf X}_t)\rangle_B &=& \int d{\bf x}d{\bf x'}d{\bf x_0} A({\bf x}) 
 \exp([t-\Delta t] G_{\bf x}^\dagger)\delta({\bf x}-{\bf x}')\exp(\Delta t [G_
{{\bf x'}}^\dagger +\epsilon\Delta G_{{\bf x}'}])\delta({\bf x}'-{\bf x}_0)p_{eq}({\bf x}_0) \\
&=& \int d{\bf x} A({\bf x}) 
 \exp([t-\Delta t] G_{\bf x}^\dagger)\exp(\Delta t [G_
{{\bf x}}^\dagger +\epsilon\Delta G_{{\bf x}}])p_{eq}({\bf x}) 
\\
&=& \int d{\bf x} A({\bf x}) 
 \exp(t G_{\bf x}^\dagger)(1-\Delta t G_{\bf x}^\dagger)(1+ \Delta t [G_
{{\bf x}}^\dagger +\epsilon\Delta G_{{\bf x}}])p_{eq}({\bf x}) +{\cal O}(\epsilon^2) \\
&=& \int d{\bf x} A({\bf x}) 
 \exp(t G_{\bf x}^\dagger)(1+ \Delta t
\epsilon\Delta G_{{\bf x}})p_{eq}({\bf x}) +{\cal O}(\epsilon^2)\\
\end{eqnarray}

\begin{equation}
\langle \delta A({\bf X}_t)\rangle = \langle \delta A({\bf X}_t)\rangle
- \langle \delta A({\bf X}_t)\rangle_B
\end{equation}

\begin{equation}
\langle \delta A({\bf X}_t)\rangle = \epsilon\Delta t\int d{\bf x} A({\bf x}) 
 \exp(t G_{\bf x}^\dagger)
\Delta G_{{\bf x}}p_{eq}({\bf x}) +{\cal O}(\epsilon^2)
\end{equation}

\begin{equation}
\langle \delta A({\bf X}_t)\rangle = \epsilon\Delta t\int d{\bf x} A({\bf x}) 
 \exp(t G_{\bf x}^\dagger)
[\frac{\Delta G_{{\bf x}}p_{eq}({\bf x})}{p_{eq}({\bf x})}]p_{eq}({\bf x}) +{\cal O}(\epsilon^2)
\end{equation}

\begin{equation}
\langle \delta A({\bf X}_t)\rangle = \Delta t\langle A({\bf X}_t) 
[\frac{\Delta G_{{\bf x}}p_{eq}({\bf X_0})}{p_{eq}({\bf X_0})}]
\end{equation}

\begin{equation}
\langle \delta A({\bf X}_t)\rangle = \int_0^t dt' \delta \chi_{AB}(t-t')\epsilon(t')
\end{equation}

\begin{equation}
\chi_{AB}(t-t') = \langle A({\bf X}_t) 
[\frac{\Delta G_{{\bf x}}p_{eq}({\bf X_{t'})}}{p_{eq}({\bf X_{t'}})}]\rangle.
\end{equation}

\begin{equation}
\Delta G^\dagger\cdot  = -\frac{1}{m}\frac{\partial}{ \partial v_i} f_i({\bf x})\cdot
\end{equation}

\begin{equation}
p_{eq}({\bf x},{\bf v})= {\cal N} \exp\left(-\frac{\beta {\bf v}^2}{2}-\beta V({\bf x})\right)
\end{equation}

\begin{equation}
\frac{[\Delta G^\dagger p_{eq}({\bf x},{\bf v})] }{p_{eq}({\bf x},{\bf v})} = -\frac{1}{mp_{eq}({\bf x},{\bf v})}\frac{\partial}{ \partial v_i} f_i({\bf x}) p_{eq}({\bf x},{\bf v}) = \beta [f_i({\bf x}) v_i] = \beta [f_i({\bf x}) \frac{dx_i}{dt}]
\end{equation}

\begin{equation}
f_i = - \frac{\partial}{\partial x_i} B({\bf x}) 
\end{equation}

\begin{equation}
\frac{[\Delta G^\dagger p_{eq}({\bf x},{\bf v})] }{p_{eq}({\bf x},{\bf v})} =
\beta \frac{d}{dt} B({\bf x})
\end{equation}


\begin{equation}
\chi_{AB}(t-t') = \beta \langle A({\bf X}_t)\frac{d}{dt'}B({\bf X}_{t'})\rangle
=-\beta \frac{d}{dt}\langle A({\bf X}_t)B({\bf X}_{t'})\rangle
\end{equation}

\begin{equation}
\chi_{AB}(t) = -\beta \theta(t)\frac{d}{dt}C_{AB}(t)
\end{equation}

\begin{equation}
C_{AB}(t) = \langle A({\bf X}_t)B({\bf X}_{0})\rangle
\end{equation}

Taking the Fourier transform of the above gives
\begin{equation}
\tilde \chi_{AB}(\omega)=\int_{-\infty}^{\infty} dt \exp(i\omega t) \chi_{AB}(t) = \int_{-\infty}^{\infty} dt \cos(\omega t) \chi_{AB}(t) + i\int_{-\infty}^{\infty} dt \sin(\omega t) \chi_{AB}(t),
\end{equation}
thus
\begin{equation}
\tilde \chi''_{AB}(\omega) = \int_{-\infty}^{\infty} dt \sin(\omega t) \chi_{AB}(t) 
\end{equation}
and using the fluctuation dissipation theorem we find
\begin{equation}
\tilde \chi''_{AB}(\omega) = -\beta \int_{-\infty}^{\infty} dt \sin(\omega t)  \theta(t)\frac{d}{dt}C_{AB}(t)
\end{equation}
Integrating by parts now gives
\begin{equation}
\tilde \chi''(\omega) = \beta\omega \int_0^\infty dt \cos(\omega t) C_{AB}(t)= \frac{\beta\omega}{2} \int_0^\infty dt \left[ \exp(i\omega t)+\exp(-i\omega t)\right]  C_{AB}(t)
\end{equation}
Now we use $C_{AB}(t)= C_{AB}(-t)$ to find
\begin{equation}
\boxed{
\tilde \chi_{AB}''(\omega) = \frac{\beta\omega}{2}\tilde C_{AB}(\omega)}.\label{fdtft}
\end{equation}
Due to causality the response function $\chi_{AB}(t)=0$ for $t<t'$. We can write
\begin{equation}
\chi_{AB}(t) =\frac{1}{2\pi}\int_{-\infty}^\infty d\omega \exp(-i\omega t) \tilde \chi(\omega),
\end{equation}
when $t<0$ the integral long the real axis can be carried out along the contour $\omega=R\exp(i\theta)$ as $R\to \infty$ and with $\theta\in[0,\pi]$ i.e above the real axis. The integral gives zero as long as there are no poles in the upper half of the complex plane.

\section{Applications of the fluctuation dissipation theorem}
In Eq. \eqref{fdt1} found a relationship between the correlation function $C(t) = \langle X(t) X(0)\rangle$ and the response to force $f$ for a particle in a Harmonic trap. In the language used above this means that $A(t)=X(t)$. Applying a force to a particle sends the energy
from $E(X)$ to $E(X)-fX$ where $f$ corresponds to the small parameter $\epsilon$. This means that $B=X$. The correlation function in the fluctuation dissipation relation is therefore
\begin{equation}
C_{AB}(t) = C_{XX}(t) = \langle X(t) X(0)\rangle,
\end{equation}
and using Eq. \eqref{fdtft} recovers Eq. \eqref{fdt1}.

As another example consider a one dimensional electrical conductor with charges $q$ 
at positions $X_i$. We consider the effect of applying a small, time dependent, electric field ${\cal E}(t)$. Doing this changes the energy of a state by $E(\{X_i\}) \to  E(\{X_i\}) - {\cal E}(t)q\sum_i X_i$. The
average electric current per unit volume is given by
\begin{equation}
j = \frac{q}{\cal V}\sum_i \frac{dX_i}{dt},
\end{equation}
where ${\cal V}$ is the volume of the system. In the notation used here $B = j$ and $A =q\sum_i X_i$ while $ {\cal E}$ plays the role of the small parameter $\epsilon$. The fluctuation dissipation theorem can be rewritten as
\begin{equation}
 \chi_{AB}(t) = -\beta\theta(t) \langle \frac{dB(t)}{dt} A(0)\rangle = \beta\theta(t) \langle B(t)\frac{dA(0)}{dt} \rangle\label{fdta}
\end{equation}
The average current is given by
\begin{equation}
j(t) = \int_{-\infty}^t  \chi_{jA}(t-t'){\cal E}(t'),
\end{equation}
and Fourier space it reads
\begin{equation}
{\tilde j}(\omega) = \tilde\chi_{jA}(\omega)\tilde{\cal E}(\omega).
\end{equation}
The frequency dependent version of Ohm's law takes the form
\begin{equation}
\tilde j(\omega) = \sigma(\omega) \tilde{\cal E}(\omega)
\end{equation}
so we can identify $\sigma(\omega)=\tilde\chi_{jA}(\omega)$ as the frequency dependent conductivity.
From the version  of fluctuation dissipation theorem in Eq. \eqref{fdta} we find
\begin{equation}
\tilde\chi_{BA}(\omega) = \int_0^\infty dt \exp(i\omega t) \beta \langle B(t)\frac{dA(0)}{dt} \rangle.
\end{equation} 
Recall that $B(t)=j(t)$, while
\begin{equation}
\frac{dA(0)}{dt} = q\sum_i \frac{dX(0)}{dt} = {\cal V}j(0).
\end{equation}
This gives
\begin{equation}
\boxed{
\sigma(\omega) = \beta {\cal V}\int_0^\infty dt \exp(i\omega t) \langle j(t)j(0) \rangle,}\label{nyq}
\end{equation} 
 the Eq. (\ref{nyq}) is known as {\em Nyquist's theorem}, it is an example of a {\em Green-Kubo formula}, which gives a transport coefficient (in this case a conductivity) as an integral
 of a correlation function. 

The problem is now to calculate
\begin{equation}
\langle j(t)j(0)\rangle = \frac{q^2}{{\cal V}^2} \sum_{ij} \langle \frac{dX_i(t)}{dt}\frac{dX_j(0)}{dt}\rangle .
\end{equation}
If we assume that the velocities of the electrons are not correlated this gives
\begin{equation}
\langle \frac{dX_i(t)}{dt}\frac{dX_j(0)}{dt}\rangle =0
\end{equation}
for $i\neq j$. We can also write using the equipartition of energy
\begin{equation}
\langle \frac{dX_i(t)}{dt}\frac{dX_i(0)}{dt}\rangle =\frac{k_B T}{m} c(t),
\end{equation}
where $c(t)$ is a decaying function with $c(0)=0$. This then gives
\begin{equation}
\langle j(t)j(0)\rangle = \frac{nq^2}{{\cal V}} \frac{k_B T}{m} c(t) ,
\end{equation}
where $n = N/{\cal V}$ is the density of electrons ($N$ being the total number) so
\begin{equation}
\sigma(\omega) = \frac{nq^2}{m}\int_0^\infty dt \exp(i\omega t) c(t)
\end{equation}
If we assume a simple exponential decay, known as {\em Debye relaxation},
\begin{equation}
c(t) =\exp(-\frac{t}{\tau_c}),
\end{equation}
where $\tau_c$ is a relaxation time, we obtain
\begin{equation}
\sigma(\omega) = \frac{nq^2}{m}\frac{\tau_c}{1-i\omega\tau_c}.
\end{equation}

















 
\chapter{Quantum Systems}
\section{Quantum correlation functions}
Consider a quantum system with Hamiltonian $H$ with eigenstates $\ket{n}$ of energy $E_n$
\begin{equation}
H\ket{n} = E_n \ket{n}. 
\end{equation}
Consider also a physical observable $A$ which is Hermitian and has eigenstates
$\ket{\alpha}$ where it has eigenvalue $\alpha$:
\begin{equation}
A\ket{\alpha} = \alpha \ket{\alpha}. 
\end{equation}
The average measured value of $A$ denoted by $\langle A\rangle$  is given by
\begin{equation}
\langle A\rangle_\psi = \sum_{\alpha} p_\alpha(\psi) \alpha
\end{equation}
where $p_\alpha(\psi)$ is the probability that the measurement gives $\alpha$ if the quantum state vector at the moment the measurement is given is $\ket{\psi}$. Thus even if the state $\ket{\psi}$ is known exactly the result of the measurement is still random.
in quantum mechanics it is important to remember that the act of measuring the observable $A$ collapses the quantum wave function in to the state $\alpha$ with probability $\alpha$.
If the wave function is given by
\begin{equation}
\ket{ \psi} = \sum_{\alpha} a_\alpha \ket{\alpha},
\end{equation} 
then the probability $p_\alpha(\psi)$ is given by
\begin{equation}
p_\alpha(\psi) = |a_\alpha^2| = |\psh{\alpha}{\psi}|^2 =\psh{\psi}{\alpha} \psh{\alpha}{\psi}
\end{equation}
and so we have
\begin{equation}
\langle A \rangle_\psi = \sum_\alpha \alpha \psh{\psi}{\alpha} \psh{\alpha}{\psi} = 
\braket{ \psi | \sum_\alpha \alpha \bra{\alpha}\ket{\alpha} |\psi}
\end{equation}
However in terms of its eigenfunctions we can write
\begin{equation}
A =\sum_\alpha \alpha \bra{\alpha}\ket{\alpha}
\end{equation}
and so we have
\begin{equation}
\langle A \rangle_\psi  = \braket{ \psi | A|\psi}.
\end{equation}
However for a system in equilibrium the state $\ket{\psi}$ is random. We can express
$\ket{\psi}$ in terms of the energy eigenstates $\ket{n}$ as
\begin{equation}
\ket{\psi}=\sum_{n} a_n \ket{n},
\end{equation}
if we measure the energy of this initial state, the probability that the wave function collapses onto the state $\ket{n}$ is given by
\begin{equation}
p_n = |a_n|^2.
\end{equation}
However if the system is in the canonical ensemble we know that 
\begin{equation}
p_n = \frac{\exp(-\beta E_n)}{Z}.
\end{equation}
This means that we can write
\begin{equation}
a_n = \sqrt{\frac{\exp(-\beta E_n)}{Z}}\exp(i\phi_n)
\end{equation}
where $\phi$ is random and not determined. However all  states with the same energy are
equally probable so this means that $\phi_n$ are uniformly distributed over $[0,2\pi]$ and we assume that they are independent. The equilibrium value of $A$ is thus given by
\begin{equation}
\langle A\rangle = \langle \langle A\rangle_\psi\rangle_{eq}
\end{equation}
where $\langle \cdot \rangle_{eq}$ denotes the average over the initial state which is taken to obey the Gibbs Boltzmann distribution. This last average is over the random coefficients
$a_n$ and we find that averaging over the phases
\begin{equation}
\langle a_n a_m\rangle_{eq}  =  \sqrt{\frac{\exp(-\beta E_n)}{Z}} \sqrt{\frac{\exp(-\beta E_m)}{Z}}
\int \frac{d\phi_n}{2\pi} \exp(i\phi_n) \int \frac{d\phi_m}{2\pi} \exp(i\phi_m)  = 0,
\end{equation}
for $m\neq n$
as both integrals over the phases are zero. Similarly we find
\begin{equation}
\langle a_n a^*_m\rangle_{eq}  =  \frac{\exp(-\beta E_n)}{Z} \delta_{nm}
\end{equation}
and also $\langle a_n^2 \rangle_{eq} = 0$.

In equilibrium therefore we find
\begin{eqnarray}
\langle A\rangle &=&  \sum_{nm}\langle a_n^* a_m\rangle_{eq} \braket{n|A|m}\nonumber \\
&=& \sum_n \frac{\exp(-\beta E_n)}{Z} \braket{n|A|n} = \sum_n \frac{1}{Z} \braket{n|\exp(-\beta H)A|n}.
\end{eqnarray}
This can be written in the compact form
\begin{equation}
\langle A\rangle = \frac{{\rm Tr}[ \exp(-\beta H)A]}{{\rm Tr} [\exp(-\beta H)]}.
\end{equation}

A correlation function in quantum mechanics is defined in terms of the Heisenberg 
picture where at time independent operator is converted into a time dependent one
by writing
\begin{equation}
A(t) = \exp(\frac{it}{\hbar}H)A\exp(-\frac{it}{\hbar}H).\label{heis}
\end{equation}
The quantum correlation function is now defined as
\begin{equation}
\langle B(t) A(t')\rangle_{\psi_0} = \braket{ \psi_0|B(t)A(t')|\psi_0},
\end{equation} 
where $\ket{\psi_0}$ is the state of the system at time $t=0$. The equilibrium correlation function is obtained by averaging over the initial conditions as before
\begin{equation}
\langle B(t) A(t')\rangle =\sum_{n,n'} \braket{ n|B(t)A(t')|n'}\langle a_n a^*_m\rangle_{eq} = \frac{1}{Z}\sum_{n,n'} \braket{ n|B(t)A(0)|n'}\exp(-\beta E_n),
\end{equation} 
which can then be written as
\begin{equation}
\langle B(t) A(t')\rangle = \frac{{\rm Tr}[ B(t)A(t')\exp(-\beta H)]}{{\rm Tr}[ \exp(-\beta H)]}.
\end{equation}
Using the properties of the trace ${\rm Tr}(AB)= {\rm Tr}(BA)$ we see that we can write
\begin{equation}
 \langle B(t) A(t')\rangle = \frac{{\rm Tr}[ \exp(-\beta H)B(t)A(t')]}{{\rm Tr}[ \exp(-\beta H)]}.
\end{equation}
The big difference with classical statistical mechanics is that the operators $A$ and $B$ will
not necessarily commute so $\langle B(t) A(t')\rangle\neq \langle A(t') B(t)\rangle$. 
However the correlation function does obey time translation invariance so
\begin{equation}
\langle B(t) A(t')\rangle = \langle B(t-t') A(0)\rangle.
\end{equation}
To show this we write out the operators $A(t')$ and $B(t)$ in terms of the Heisenberg representation of Eq. \eqref{heis} 
\begin{eqnarray}
&&{\rm Tr} \exp(-\beta H) \exp\left[ \frac{i}{\hbar}tH\right] B \exp\left[ -\frac{i}{\hbar}tH\right]\exp\left[ \frac{i}{\hbar}t'H\right]A \exp\left[- \frac{i}{\hbar}tH\right]\nonumber \\
&=&{\rm Tr} \exp(-\beta H) \exp\left[ \frac{i}{\hbar}tH\right] B \exp\left[ -\frac{i}{\hbar}(t-t')H\right]A \exp\left[- \frac{i}{\hbar}t'H\right]\nonumber \\
&=& {\rm Tr} \exp\left[- \frac{i}{\hbar}t'H\right]
\exp(-\beta H) \exp\left[ \frac{i}{\hbar}tH\right] B \exp\left[ -\frac{i}{\hbar}(t-t')H\right]A  \nonumber \\
&=&{\rm Tr} 
\exp(-\beta H) \exp\left[ \frac{i}{\hbar}(t-t')H\right] B \exp\left[ -\frac{i}{\hbar}(t-t')H\right]A,
\end{eqnarray}
where in the above we have used the property of the trace that ${\rm Tr}(AB)= {\rm Tr}(BA)$ and also the property that $\exp\left[- \frac{i}{\hbar}t'H\right]
\exp(-\beta H)=\exp(-\beta H)\exp\left[- \frac{i}{\hbar}t'H\right]$.




Now let us consider making two measurements in a system which at time $t=0$ have initial quantum state $\ket{\psi_0}$ at time $t=0$ we measure $A$ and then at time $t$ we measure
another observable $C$ with quantum eigenfunctions $\ket{\gamma}$ with eigenvalues $\gamma$. The correlation function
\begin{equation}
\langle C(t) A(0)\rangle_{\psi_0} = \sum_{\alpha,\gamma} \gamma \alpha \ p(\gamma,t;\alpha,0)
\end{equation}
where $p_(\gamma,t;\alpha,0)$ is the probability that the first measurement collapses the wave function onto the state $\ket{\alpha}$ and the second onto the state $\ket{\gamma}$.
The probability the first measurement gives $\alpha$ is simply given by
\begin{equation}
p(\alpha,0) = |\psh{\psi_0}{\alpha}|^2 = \psh{\psi_0}{\alpha}\psh{\alpha}{\psi_0}
\end{equation}
However if the system collapses into the state $\ket{\alpha}$ the wave function at time $t$ 
is given by solving the Schr\"odinger equation with initial condition $\ket{\psi(0)} = \ket{\alpha}$ to obtain
\begin{equation}
\ket{\psi(t)} = \exp(-\frac{it}{\hbar}H)\ket{\alpha},
\end{equation}
the probability that the second measurement at time $t$ gives $\gamma$ is then simply
\begin{equation}
p(\gamma,t|\alpha,0) = |\psh{\psi(t)}{\gamma}|^2 = |\braket{\gamma |\exp(-\frac{it}{\hbar}H)|\alpha}|^2 = \braket{\gamma |\exp(-\frac{it}{\hbar}H)|\alpha}\braket{\alpha |\exp(\frac{it}{\hbar}H)|\gamma}.
\end{equation}
Putting all this together then gives
\begin{eqnarray}
\langle C(t) A(0)\rangle_{\psi_0} &=& \sum_{\alpha,\gamma} \gamma \alpha \ p(\gamma,t;\alpha,0)= \sum_{\alpha,\gamma} \ \gamma \alpha \ p(\gamma,t|\alpha,0)p(\alpha,0)\nonumber
\\
&=& \sum_{\alpha,\gamma} \gamma \alpha \ \braket{\alpha |\exp(\frac{it}{\hbar}H)|\gamma} \braket{\gamma |\exp(-\frac{it}{\hbar}H)|\alpha}\psh{\psi_0}{\alpha}\psh{\alpha}{\psi_0}\nonumber \\
&=& \sum_{\alpha}  \alpha  \braket{\alpha |\exp(\frac{it}{\hbar}H) C \exp(-\frac{it}{\hbar}H)|\alpha}\psh{\psi_0}{\alpha}\psh{\alpha}{\psi_0}.
\end{eqnarray}
Now we average over the initial equilibrium conditions and can show 
\begin{eqnarray}
\langle \psh{\psi_0}{\alpha}\psh{\alpha}{\psi_0}\rangle_{eq} =\frac{1}{Z} \sum_n \exp(-\beta E_n)\psh{n}{\alpha}\psh{\alpha}{n}.
\end{eqnarray}
This then gives
\begin{equation}
\langle C(t) A(0)\rangle= \frac{1}{Z}\sum_{\alpha,n}  \alpha  \braket{\alpha |\exp(\frac{it}{\hbar}H) C \exp(-\frac{it}{\hbar}H)|\alpha}\exp(-\beta E_n)\psh{n}{\alpha}\psh{\alpha}{n}.
\end{equation}
We now write
\begin{equation}
\sum_n \exp(-\beta E_n)\psh{n}{\alpha}\psh{\alpha}{n}= \braket{\alpha |\exp(-\beta H)|\alpha},
\end{equation}
so
\begin{equation}
\langle C(t) A(0)\rangle= \frac{1}{Z}\sum_{\alpha,n}  \alpha  \braket{\alpha |\exp(\frac{it}{\hbar}H) C \exp(-\frac{it}{\hbar}H)|\alpha}\braket{\alpha |\exp(-\beta H)|\alpha},\label{qt2}
\end{equation}
and thus
\begin{eqnarray}
\langle C(t) A(0)\rangle= \frac{1}{Z}\sum_{\alpha}   \braket{\alpha |\exp(\frac{it}{\hbar}H) C \exp(-\frac{it}{\hbar}H)A\exp(-\beta H)|\alpha},\label{qt1}.
\end{eqnarray}
where you can see that Eq. \eqref{qt2} is equal to Eq. \eqref{qt1} by writing the operator $A$ as
\begin{equation}
A = \sum_{\alpha'}\alpha'\ket{\alpha'}\bra{\alpha'}.
\end{equation}
 \section{The Caldeira-Legget Model}
The Caldeira-Legget model provides a microscopic derivation of the Langevin equation 
based on the interaction of a particle with a bath of simple Harmonic oscillators. We consider a particle of mass $M$ with position $X$ and momentum $P$ interaction with an ensemble of oscillators with positions $q_k$, momenta $p_k$ and masses $m_k$ and frequencies $\omega_k$. The overall Hamiltonian is given by
\begin{equation}
H= H_P + H_I + H_B + H_{CT},
\end{equation}
where $H_P$ is the Hamiltonian of the free particle
\begin{equation}
H_P = \frac{P^2}{2M} + V(X)
\end{equation}
where $V$ is an external potential acting on the particle. The term $H_I$ is a linear  interaction between the particle and the oscillators
\begin{equation}
H_I = X\sum_k C_k q_k,
\end{equation}
while $H_B$ is the Hamiltonian of the bath or reservoir of oscillators
\begin{equation}
H_B = \sum_k \frac{p_k^2}{2m_k} + \frac{m_k \omega_k^2 q_k^2}{2}.
\end{equation}
The term $H_{CT}$ is a counter term necessary for the model to make mathematical sense and in given by
\begin{equation}
H_{CT} = -X^2 \sum_k \frac{C_k}{2m_k \omega_k^2}
\end{equation}
